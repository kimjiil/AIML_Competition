{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6669d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 27 11:08:48 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:24:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8     2W / 250W |     18MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:29:00.0 Off |                  N/A |\n",
      "| 30%   43C    P2    53W / 250W |   4731MiB / 11264MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:2E:00.0 Off |                  N/A |\n",
      "| 30%   29C    P8    17W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:33:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8     1W / 250W |  10319MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:60:00.0 Off |                  N/A |\n",
      "| 30%   21C    P8    21W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:6F:00.0 Off |                  N/A |\n",
      "| 30%   21C    P8     8W / 250W |   6017MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:74:00.0 Off |                  N/A |\n",
      "| 29%   27C    P8     1W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  Off  | 00000000:79:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8     7W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      4009      G   /usr/bin/gnome-shell                4MiB |\n",
      "|    1   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2910083      C   ...3/envs/pyKYH39/bin/python     4723MiB |\n",
      "|    2   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   2857005      C   ...3/envs/torch1a/bin/python    10311MiB |\n",
      "|    4   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A   2830910      C   ...a3/envs/py39_0/bin/python     2003MiB |\n",
      "|    5   N/A  N/A   2830928      C   ...a3/envs/py39_0/bin/python     2003MiB |\n",
      "|    5   N/A  N/A   2830948      C   ...a3/envs/py39_0/bin/python     2003MiB |\n",
      "|    6   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200a39ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kji/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_110853-2qttqj49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2qttqj49\" target=\"_blank\">deep-valley-35</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2qttqj49?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbfd9820b20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "import cv2, PIL\n",
    "import os, glob\n",
    "import csv, platform\n",
    "\n",
    "current_os = platform.system()\n",
    "if current_os == \"Linux\":\n",
    "    _path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "    cfg = {\n",
    "        'device': \"cuda:5\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 32,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 4,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "elif current_os == \"Windows\":\n",
    "    _path = 'D:/git_repos/samsung_sem'\n",
    "    cfg = {\n",
    "        'device': \"cuda:0\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 4,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 0,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login(key='0322000365224d30ef0694f60237c68767290052')\n",
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d27278",
   "metadata": {},
   "source": [
    "# Cycle Gan Model,  Generator:Resnet, Discriminator:PatchGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e22ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(resnet_block, self).__init__()\n",
    "\n",
    "        _resnet_block = [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim)\n",
    "        ]\n",
    "\n",
    "        self.layer = nn.Sequential(*_resnet_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x) + x\n",
    "        return out\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.donw_sampling_layer1 = nn.Sequential(\n",
    "            # down sampling\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.donw_sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.res_block = nn.Sequential(*[resnet_block(64) for i in range(3)])\n",
    "\n",
    "        self.up_samplig_layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.up_samplig_layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(16, input_ch, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.model(x)\n",
    "        out1 = self.init_layer(x)\n",
    "        out2 = self.donw_sampling_layer1(out1)\n",
    "        out3 = self.donw_sampling_layer2(out2)\n",
    "        out4 = self.res_block(out3)\n",
    "        out5 = self.up_samplig_layer1(out4)\n",
    "        out6 = self.up_samplig_layer2(out5)\n",
    "        out7 = self.output_layer(out6)\n",
    "        return out7\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "class PatchGanDiscriminator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(PatchGanDiscriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, stride=1, padding=1, padding_mode='replicate'),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1, bias=True),  # 1\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=True),  # 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'wgan_gp':\n",
    "            self.loss = None\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            if target_is_real:\n",
    "                target_tensor = self.real_label  # .to(self.device)\n",
    "            else:\n",
    "                target_tensor = self.fake_label  # .to(self.device)\n",
    "\n",
    "            target_tensor = target_tensor.expand_as(prediction)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "\n",
    "def _gradient_penalty(netD, real_data, fake_data, type=\"mixed\", constant=1.0, lambda_gp=10.0):\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':  # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1, device=real_data.device)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(\n",
    "                *real_data.shape)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(real_data.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "class cycleGAN_model(nn.Module):\n",
    "    def __init__(self, input_ch=3,\n",
    "                 optim_lr=0.0002,\n",
    "                 gan_mode='lsgan',\n",
    "                 guided=False):\n",
    "        import itertools\n",
    "\n",
    "        super(cycleGAN_model, self).__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.guided = guided\n",
    "\n",
    "        self.Gen = nn.ModuleDict({\n",
    "            'A': ResnetGenerator(input_ch),\n",
    "            'B': ResnetGenerator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Gen['A'], log='all')\n",
    "        # wandb.watch(self.Gen['B'], log='all')\n",
    "\n",
    "        self.Dis = nn.ModuleDict({\n",
    "            'A': PatchGanDiscriminator(input_ch),\n",
    "            'B': PatchGanDiscriminator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Dis['A'], log='all')\n",
    "        # wandb.watch(self.Dis['B'], log='all')\n",
    "\n",
    "        self.optimizer = {\n",
    "            'G': torch.optim.Adam(itertools.chain(self.Gen['A'].parameters(), self.Gen['B'].parameters()), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_A': torch.optim.Adam(self.Dis['A'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_B': torch.optim.Adam(self.Dis['B'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999))\n",
    "        }\n",
    "\n",
    "        self.schedular = {\n",
    "            'G': torch.optim.lr_scheduler.LambdaLR(self.optimizer['G'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_A': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_A'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_B': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_B'], lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "        }\n",
    "\n",
    "        self.criterion = nn.ModuleDict({\n",
    "            'cycle': nn.L1Loss(),\n",
    "            'idt': nn.L1Loss(),\n",
    "            'gan': GANLoss(self.gan_mode),\n",
    "            'mse': nn.MSELoss(),\n",
    "            'guided': nn.L1Loss()\n",
    "        })\n",
    "\n",
    "        self.lambda_idt = 0.5\n",
    "        self.lambda_A = 10.0\n",
    "        self.lambda_B = 10.0\n",
    "\n",
    "    def forward(self, data_A, data_B, mode: str):\n",
    "        if mode == 'gen':\n",
    "            A_out = self.Gen['A'](data_A)\n",
    "            B_out = self.Gen['B'](data_B)\n",
    "        elif mode == 'dis':\n",
    "            A_out = self.Dis['A'](data_A)\n",
    "            B_out = self.Dis['B'](data_B)\n",
    "        else:\n",
    "            raise None\n",
    "        return A_out, B_out\n",
    "\n",
    "    def model_train_discriminator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', True)\n",
    "\n",
    "        self.optimizer['D_B'].zero_grad()\n",
    "\n",
    "        pred_real_B, pred_real_A = self(real_B, real_A, 'dis')  # netA netB\n",
    "        pred_fake_B, pred_fake_A = self(fake_B.detach(), fake_A.detach(), 'dis')\n",
    "\n",
    "        # Discriminator B update\n",
    "        loss_D_B_Real = self.criterion['gan'](pred_real_A, True)\n",
    "        loss_D_B_fake = self.criterion['gan'](pred_fake_A, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_B = (loss_D_B_fake + loss_D_B_Real) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_B = _gradient_penalty(self.Dis['B'], real_A, fake_A.detach())\n",
    "            loss_D_B = loss_D_B_fake + loss_D_B_Real + gradient_penalty_B[0]\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        self.optimizer['D_B'].step()\n",
    "\n",
    "        # Discriminator A update\n",
    "        self.optimizer['D_A'].zero_grad()\n",
    "\n",
    "        loss_D_A_Real = self.criterion['gan'](pred_real_B, True)\n",
    "        loss_D_A_fake = self.criterion['gan'](pred_fake_B, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_A = (loss_D_A_Real + loss_D_A_fake) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_A = _gradient_penalty(self.Dis['A'], real_B, fake_B.detach())\n",
    "            loss_D_A = loss_D_A_Real + loss_D_A_fake + gradient_penalty_A[0]\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        self.optimizer['D_A'].step()\n",
    "\n",
    "        loss_dic = {'dis_a': loss_D_A.item(),\n",
    "                    'dis_b': loss_D_B.item()}\n",
    "\n",
    "        return loss_dic\n",
    "\n",
    "    def model_train_generator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "        rec_B, rec_A = self(fake_A, fake_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', False)\n",
    "        self.optimizer['G'].zero_grad()\n",
    "\n",
    "        idt_A, idt_B = self(real_B, real_A, 'gen')\n",
    "\n",
    "        loss_idt_A = self.criterion['idt'](idt_A, real_B) * self.lambda_B * self.lambda_idt\n",
    "        loss_idt_B = self.criterion['idt'](idt_B, real_A) * self.lambda_A * self.lambda_idt\n",
    "\n",
    "        dis_A_fake_B, dis_B_fake_A = self(fake_B, fake_A, 'dis')  # dis_A(fake_B) / dis_B(fake_A)\n",
    "\n",
    "        loss_G_A = self.criterion['gan'](dis_A_fake_B, True)\n",
    "        loss_G_B = self.criterion['gan'](dis_B_fake_A, True)\n",
    "\n",
    "        loss_cycle_A = self.criterion['cycle'](rec_A, real_A) * self.lambda_A\n",
    "        loss_cycle_B = self.criterion['cycle'](rec_B, real_B) * self.lambda_B\n",
    "\n",
    "        # Guied Loss (paired)\n",
    "        if self.guided:\n",
    "            loss_guided_A = self.criterion['guided'](fake_B, real_B)\n",
    "            loss_guided_B = self.criterion['guided'](fake_A, real_A)\n",
    "        else:\n",
    "            loss_guided_A = 0\n",
    "            loss_guided_B = 0\n",
    "        ##########\n",
    "\n",
    "        loss_Gen = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B + loss_guided_A + loss_guided_B\n",
    "        loss_Gen.backward()\n",
    "\n",
    "        self.optimizer['G'].step()\n",
    "\n",
    "        loss_dic = {'gen': loss_Gen.item()}\n",
    "\n",
    "        inference_image = {\n",
    "            'real_a': real_A,\n",
    "            'real_b': real_B,\n",
    "            'atob_fake': fake_B,\n",
    "            'btoa_fake': fake_A,\n",
    "            'rec_a': rec_A,\n",
    "            'rec_b': rec_B\n",
    "        }\n",
    "\n",
    "        return loss_dic, {key: self.tensortonp(inference_image[key]) for key in inference_image}\n",
    "\n",
    "    def model_valid(self, real_A, real_B):\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "            true = (real_B * 255).type(torch.uint8).float()\n",
    "            fake_true = (fake_B * 255).type(torch.uint8).float()\n",
    "            rmse_loss = torch.sqrt(self.criterion['mse'](fake_true, true))\n",
    "\n",
    "        img_dict = {\n",
    "            'real_A': real_A,\n",
    "            'fake_B': fake_B,\n",
    "\n",
    "            'real_B': real_B,\n",
    "            'fake_A': fake_A,\n",
    "        }\n",
    "\n",
    "        return rmse_loss.item(), {key: self.tensortonp(img_dict[key]) for key in img_dict}\n",
    "\n",
    "    def tensortonp(self, tensor):\n",
    "        return (tensor.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    def set_requires_grad(self, net_type='dis', mode=True):\n",
    "        if net_type == 'gen':\n",
    "            net_dic = self.Gen\n",
    "        elif net_type == 'dis':\n",
    "            net_dic = self.Dis\n",
    "\n",
    "        for key in net_dic:\n",
    "            net_dic[key].set_requires_grad(mode)\n",
    "\n",
    "    def schedular_step(self):\n",
    "        self.schedular['G'].step()\n",
    "        self.schedular['D_A'].step()\n",
    "        self.schedular['D_B'].step()\n",
    "\n",
    "    def model_save(self, PATH):\n",
    "        temp_dict = {}\n",
    "        key_list = [key for key in self.__dict__.keys() if not '_' in key[0]]\n",
    "        key_list.extend([key for key in self.__dict__['_modules'].keys()])\n",
    "\n",
    "        for key in key_list:\n",
    "            if hasattr(self, key):\n",
    "                value = getattr(self, key)\n",
    "                if isinstance(value, dict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = {}\n",
    "                    for sub_key in value.keys():\n",
    "                        if not sub_key in temp_dict[key]:\n",
    "                            temp_dict[key][sub_key] = value[sub_key].state_dict()\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value.state_dict()\n",
    "                else:\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value\n",
    "\n",
    "        torch.save(temp_dict, PATH)\n",
    "\n",
    "    def model_load(self, PATH, device):\n",
    "        state_dict = torch.load(PATH, map_location=device)\n",
    "\n",
    "        for cls_key in state_dict.keys():\n",
    "            if hasattr(self, cls_key):\n",
    "                value = getattr(self, cls_key)\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key in value.keys():\n",
    "                        value[sub_key].load_state_dict(state_dict[cls_key][sub_key])\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    value.load_state_dict(state_dict[cls_key])\n",
    "                else:\n",
    "                    setattr(self, cls_key, state_dict[cls_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980f7e6",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5f4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list(abs_path):\n",
    "    # abs_path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "\n",
    "    # Dataset path\n",
    "    sim_depth_path = os.path.join(abs_path, 'simulation_data/Depth')\n",
    "    sim_sem_path = os.path.join(abs_path, 'simulation_data/SEM')\n",
    "\n",
    "    train_path = os.path.join(abs_path, 'train')\n",
    "\n",
    "    # only Test\n",
    "    test_path = os.path.join(abs_path, 'test/SEM')\n",
    "\n",
    "    sim_depth_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_depth_path):\n",
    "        if not case in sim_depth_img_path_dic:\n",
    "            sim_depth_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_depth_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_depth_path, case, folder, '*.png'))\n",
    "            for img in img_list:\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "\n",
    "    sim_sem_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_sem_path):\n",
    "        if not case in sim_sem_img_path_dic:\n",
    "            sim_sem_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_sem_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_sem_path, case, folder, '*.png'))\n",
    "            sim_sem_img_path_dic[case].extend(img_list)\n",
    "\n",
    "    train_avg_depth = dict()\n",
    "    with open(os.path.join(train_path, \"average_depth.csv\"), 'r') as csvfile:\n",
    "        temp = csv.reader(csvfile)\n",
    "        for idx, line in enumerate(temp):\n",
    "            if idx > 0:\n",
    "                depth_key, site_key = line[0].split('_site')\n",
    "                depth_key = depth_key.replace(\"d\", \"D\")\n",
    "                site_key = \"site\" + site_key\n",
    "                if not depth_key in train_avg_depth:\n",
    "                    train_avg_depth[depth_key] = dict()\n",
    "\n",
    "                train_avg_depth[depth_key][site_key] = float(line[1])\n",
    "\n",
    "    train_img_path_dic = dict()\n",
    "    for depth in os.listdir(os.path.join(train_path, \"SEM\")):\n",
    "        if not depth in train_img_path_dic:\n",
    "            train_img_path_dic[depth] = []\n",
    "        for site in os.listdir(os.path.join(train_path, \"SEM\", depth)):\n",
    "            img_list = glob.glob(os.path.join(train_path, \"SEM\", depth, site, \"*.png\"))\n",
    "            train_img_path_dic[depth].extend([[temp_img, train_avg_depth[depth][site]] for temp_img in img_list])\n",
    "\n",
    "    test_img_path_list = glob.glob(os.path.join(test_path, \"*.png\"))\n",
    "\n",
    "    result_dic = dict()\n",
    "    result_dic['sim'] = dict()\n",
    "    result_dic['sim']['sem'] = sim_sem_img_path_dic\n",
    "    result_dic['sim']['depth'] = sim_depth_img_path_dic\n",
    "    result_dic['train'] = train_img_path_dic\n",
    "    result_dic['test'] = np.array(test_img_path_list)\n",
    "    result_dic['train_avg_depth'] = train_avg_depth\n",
    "\n",
    "    return result_dic\n",
    "\n",
    "result_dic = get_img_list(cfg['db_path'])\n",
    "\n",
    "'''\n",
    "train sem ->  sim sem -> sim depth\n",
    "\n",
    "case 별로 dataset을 나눠야됨.\n",
    "'''\n",
    "\n",
    "class gan_dataset(Dataset):\n",
    "    def __init__(self, a_data_path, b_data_path, transform=None):\n",
    "        super(gan_dataset, self).__init__()\n",
    "        self.a_data_path = a_data_path\n",
    "        self.b_data_path = b_data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.a_size = len(a_data_path)\n",
    "        self.b_size = len(b_data_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.a_size > self.b_size:\n",
    "            a_idx = idx\n",
    "            b_idx = idx % self.b_size\n",
    "        else:\n",
    "            a_idx = idx % self.a_size\n",
    "            b_idx = idx\n",
    "        if isinstance(self.a_data_path[a_idx], str):\n",
    "            a_path = self.a_data_path[a_idx]\n",
    "        elif isinstance(self.a_data_path[a_idx], list):\n",
    "            a_path = self.a_data_path[a_idx][0]\n",
    "\n",
    "        if isinstance(self.b_data_path[b_idx], str):\n",
    "            b_path = self.b_data_path[b_idx]\n",
    "        elif isinstance(self.b_data_path[b_idx], list):\n",
    "            b_path = self.b_data_path[b_idx][0]\n",
    "\n",
    "        a_img = PIL.Image.open(a_path).convert(\"L\")\n",
    "        b_img = PIL.Image.open(b_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            a_img = self.transform(a_img)\n",
    "            b_img = self.transform(b_img)\n",
    "\n",
    "        a_img = (np.array(a_img) / 255.)\n",
    "        a_img = a_img.reshape(1, *a_img.shape).astype(np.float32)\n",
    "        b_img = (np.array(b_img) / 255.)\n",
    "        b_img = b_img.reshape(1, *b_img.shape).astype(np.float32)\n",
    "\n",
    "        return a_img, b_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.a_data_path), len(self.b_data_path))\n",
    "\n",
    "def create_dataloader(a_key, b_key, t_ratio, result_dic, case=1):\n",
    "    if 'sim' in a_key:\n",
    "        a_list = result_dic['sim'][a_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        a_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "    if 'sim' in b_key:\n",
    "        b_list = result_dic['sim'][b_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        b_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "\n",
    "    horizon_transform = transforms.RandomHorizontalFlip(1.0)\n",
    "    rotate_transform = transforms.RandomRotation((180, 180))\n",
    "    vertical_transform = transforms.RandomVerticalFlip(1.0)\n",
    "\n",
    "    a_train_data_size = int(len(a_list) * t_ratio)\n",
    "    b_train_data_size = int(len(b_list) * t_ratio)\n",
    "\n",
    "    train_dataset = gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], None) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], vertical_transform)\n",
    "\n",
    "    valid_dataset = gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], None) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], vertical_transform)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=True), \\\n",
    "           DataLoader(valid_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9442a19",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191c17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def valid(model, valid_dataloader, device):\n",
    "    rmse_list = []\n",
    "    img_mean_list = []\n",
    "    for step_i, data_tuple in enumerate(valid_dataloader):\n",
    "        real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "        real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "        rmse_loss, img_dict = model.model_valid(real_a, real_b)\n",
    "        rmse_list.append(rmse_loss)\n",
    "        if step_i == 0:\n",
    "            img_list = [img_dict[key][0][0] for key in img_dict]\n",
    "            img_list = [wandb.Image(PIL.Image.fromarray(np.concatenate((img_list[i], img_list[i+1]), axis=-1)).convert('L'), caption=key)\n",
    "                        for i, key in enumerate(img_dict.keys()) if i % 2 == 0]\n",
    "            wandb.log({\n",
    "                \"example image\": img_list\n",
    "            })\n",
    "        \n",
    "    \n",
    "        img_mean_list.extend(list(np.mean(img_dict['fake_B'], axis=(1,2,3))))\n",
    "        \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.hist(img_mean_list, bins=100, density=True, alpha=0.5, color=plt.cm.tab20c(1))\n",
    "    plt.xlim(80, 200)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({\n",
    "        'plot': wandb.Image(fig)\n",
    "    })\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "def training(case, epochs, device, type, guided ,checkpoint_path=None):\n",
    "    best_rmse_loss = 9999\n",
    "    critic_iter = 5\n",
    "    best_epoch = 0\n",
    "\n",
    "    if type == 'semtodepth':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'sim_depth'\n",
    "    elif type == 'simtotrain':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'train'\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloader(a_key=a_key,\n",
    "                                                           b_key=b_key,\n",
    "                                                           t_ratio=0.8,\n",
    "                                                           result_dic=result_dic,\n",
    "                                                           case=case)\n",
    "\n",
    "    model = cycleGAN_model(1, optim_lr=0.0002, gan_mode='wgan_gp', guided=guided)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        model.model_load(checkpoint_path, device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_list = [[], [], []]\n",
    "        for step_i, data_tuple in enumerate(train_dataloader):\n",
    "            real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "            real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "            dis_loss = model.model_train_discriminator(real_a, real_b)\n",
    "            loss_list[1].append(dis_loss['dis_a'])\n",
    "            loss_list[2].append(dis_loss['dis_b'])\n",
    "            if step_i % critic_iter == 0:\n",
    "                gen_loss, img_dic = model.model_train_generator(real_a, real_b)\n",
    "                loss_list[0].append(gen_loss['gen'])\n",
    "\n",
    "                wandb.log({\n",
    "                    'Gen_step_loss': gen_loss,\n",
    "                    'Dis_A_step_loss': dis_loss['dis_a'],\n",
    "                    'Dis_B_step_loss': dis_loss['dis_b']\n",
    "                })\n",
    "\n",
    "            \n",
    "        rmse_loss = valid(model, valid_dataloader, device)\n",
    "        print(f'epoch - {epoch}, gen loss - {gen_loss}, rmse loss - {rmse_loss}')\n",
    "        wandb.log({\n",
    "            'Gen_loss': np.mean(loss_list[0]),\n",
    "            'Dis_A_loss': np.mean(loss_list[1]),\n",
    "            'Dis_B_loss': np.mean(loss_list[2]),\n",
    "            'learning_rate': model.schedular['G'].get_last_lr(),\n",
    "            'rmse_loss': rmse_loss\n",
    "        })\n",
    "\n",
    "        if best_rmse_loss > rmse_loss:\n",
    "            best_rmse_loss = rmse_loss\n",
    "            model.model_save(f'./case{case}_t({type})_best_model.pth')\n",
    "\n",
    "        model.schedular_step()\n",
    "    print(f'training end, best epoch - {best_epoch}, best valid rmse loss - {best_rmse_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1495f",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #1 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdcff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.6897220015525818}, rmse loss - 10.262341294341422\n",
      "epoch - 1, gen loss - {'gen': 0.9290295839309692}, rmse loss - 9.442021289874706\n",
      "epoch - 2, gen loss - {'gen': 1.0008044242858887}, rmse loss - 9.016276080230066\n",
      "epoch - 3, gen loss - {'gen': 1.1876195669174194}, rmse loss - 8.641483055709474\n",
      "epoch - 4, gen loss - {'gen': 0.9760961532592773}, rmse loss - 8.461139863267595\n",
      "epoch - 5, gen loss - {'gen': 0.7497209310531616}, rmse loss - 8.182416282896627\n",
      "epoch - 6, gen loss - {'gen': 0.5105769634246826}, rmse loss - 8.16481893176962\n",
      "epoch - 7, gen loss - {'gen': 0.7134808301925659}, rmse loss - 8.021302988168499\n",
      "epoch - 8, gen loss - {'gen': 0.6867778301239014}, rmse loss - 7.914659805843311\n",
      "epoch - 9, gen loss - {'gen': 0.9704908728599548}, rmse loss - 7.820144692470227\n",
      "epoch - 10, gen loss - {'gen': 0.7339076995849609}, rmse loss - 7.777795176224515\n",
      "epoch - 11, gen loss - {'gen': 0.6545236706733704}, rmse loss - 7.840254746240004\n",
      "epoch - 12, gen loss - {'gen': 0.5399556756019592}, rmse loss - 7.665868790827115\n",
      "epoch - 13, gen loss - {'gen': 0.4414660334587097}, rmse loss - 7.728357479581094\n",
      "epoch - 14, gen loss - {'gen': 0.5960054993629456}, rmse loss - 7.777961509694032\n",
      "epoch - 15, gen loss - {'gen': 0.5346986055374146}, rmse loss - 7.658271061978217\n",
      "epoch - 16, gen loss - {'gen': 0.5371488332748413}, rmse loss - 7.689207713102503\n",
      "epoch - 17, gen loss - {'gen': 0.5188764333724976}, rmse loss - 7.688448973247485\n",
      "epoch - 18, gen loss - {'gen': 0.43897613883018494}, rmse loss - 7.612891334449233\n",
      "epoch - 19, gen loss - {'gen': 0.6384178400039673}, rmse loss - 7.617251119490479\n",
      "epoch - 20, gen loss - {'gen': 0.4223549962043762}, rmse loss - 7.64421475373511\n",
      "epoch - 21, gen loss - {'gen': 0.7348949909210205}, rmse loss - 7.682724761347049\n",
      "epoch - 22, gen loss - {'gen': 0.6104030013084412}, rmse loss - 7.6412986701704915\n",
      "epoch - 23, gen loss - {'gen': 0.5111749768257141}, rmse loss - 7.644447140588092\n",
      "epoch - 24, gen loss - {'gen': 0.608685314655304}, rmse loss - 7.660008140595637\n",
      "epoch - 25, gen loss - {'gen': 0.538867712020874}, rmse loss - 7.633622063921826\n",
      "epoch - 26, gen loss - {'gen': 0.7318858504295349}, rmse loss - 7.626894994855367\n",
      "epoch - 27, gen loss - {'gen': 0.4982016086578369}, rmse loss - 7.673633417960023\n",
      "epoch - 28, gen loss - {'gen': 0.7351390719413757}, rmse loss - 7.651493471047095\n",
      "epoch - 29, gen loss - {'gen': 0.7602669596672058}, rmse loss - 7.667401761146489\n",
      "epoch - 30, gen loss - {'gen': 0.6080026030540466}, rmse loss - 7.648700324811618\n",
      "epoch - 31, gen loss - {'gen': 0.48534920811653137}, rmse loss - 7.656836793431497\n",
      "epoch - 32, gen loss - {'gen': 0.7029957175254822}, rmse loss - 7.655241527240655\n",
      "epoch - 33, gen loss - {'gen': 0.736897885799408}, rmse loss - 7.654479405096976\n",
      "epoch - 34, gen loss - {'gen': 0.5507946014404297}, rmse loss - 7.672826230306027\n",
      "epoch - 35, gen loss - {'gen': 0.6705659627914429}, rmse loss - 7.626475855433193\n",
      "epoch - 36, gen loss - {'gen': 0.6353065371513367}, rmse loss - 7.63792801871071\n",
      "epoch - 37, gen loss - {'gen': 0.539273738861084}, rmse loss - 7.612358741214795\n",
      "epoch - 38, gen loss - {'gen': 0.6367734670639038}, rmse loss - 7.624721804228216\n",
      "epoch - 39, gen loss - {'gen': 0.527733564376831}, rmse loss - 7.622464861817026\n",
      "epoch - 40, gen loss - {'gen': 0.42754724621772766}, rmse loss - 7.651939845173121\n",
      "epoch - 41, gen loss - {'gen': 0.49417468905448914}, rmse loss - 7.6470534731101285\n",
      "epoch - 42, gen loss - {'gen': 0.5106481313705444}, rmse loss - 7.620613195359487\n",
      "epoch - 43, gen loss - {'gen': 0.4208751916885376}, rmse loss - 7.615578228257239\n",
      "epoch - 44, gen loss - {'gen': 0.5194477438926697}, rmse loss - 7.641064023179761\n",
      "epoch - 45, gen loss - {'gen': 0.4883495271205902}, rmse loss - 7.592844101775616\n",
      "epoch - 46, gen loss - {'gen': 0.4780321717262268}, rmse loss - 7.620645860024484\n",
      "epoch - 47, gen loss - {'gen': 0.5568549633026123}, rmse loss - 7.660537739081576\n",
      "epoch - 48, gen loss - {'gen': 0.46365994215011597}, rmse loss - 7.637613948860731\n",
      "epoch - 49, gen loss - {'gen': 0.5515592098236084}, rmse loss - 7.626445316300621\n",
      "epoch - 50, gen loss - {'gen': 0.5607195496559143}, rmse loss - 7.635924414954942\n",
      "epoch - 51, gen loss - {'gen': 0.5131929516792297}, rmse loss - 7.6653912986776485\n",
      "epoch - 52, gen loss - {'gen': 0.5037456750869751}, rmse loss - 7.6135795164812095\n",
      "epoch - 53, gen loss - {'gen': 0.512083113193512}, rmse loss - 7.6275995807014265\n",
      "epoch - 54, gen loss - {'gen': 0.49988698959350586}, rmse loss - 7.631795887577578\n",
      "epoch - 55, gen loss - {'gen': 0.4053020179271698}, rmse loss - 7.61979105199835\n",
      "epoch - 56, gen loss - {'gen': 0.3877766728401184}, rmse loss - 7.61809970575945\n",
      "epoch - 57, gen loss - {'gen': 0.4114163815975189}, rmse loss - 7.652380652533246\n",
      "epoch - 58, gen loss - {'gen': 0.42322802543640137}, rmse loss - 7.636854920440054\n",
      "epoch - 59, gen loss - {'gen': 0.36622270941734314}, rmse loss - 7.6121429429283\n",
      "epoch - 60, gen loss - {'gen': 0.41151779890060425}, rmse loss - 7.622399678529408\n",
      "epoch - 61, gen loss - {'gen': 0.4251604676246643}, rmse loss - 7.595702792005786\n",
      "epoch - 62, gen loss - {'gen': 0.44279226660728455}, rmse loss - 7.644646141361926\n",
      "epoch - 63, gen loss - {'gen': 0.43304169178009033}, rmse loss - 7.618851059477268\n",
      "epoch - 64, gen loss - {'gen': 0.3189374804496765}, rmse loss - 7.6440707590307255\n",
      "epoch - 65, gen loss - {'gen': 0.3387799561023712}, rmse loss - 7.617846846140619\n",
      "epoch - 66, gen loss - {'gen': 0.4144376218318939}, rmse loss - 7.622229232119458\n",
      "epoch - 67, gen loss - {'gen': 0.3713032603263855}, rmse loss - 7.62062709313917\n",
      "epoch - 68, gen loss - {'gen': 0.33780401945114136}, rmse loss - 7.613850401776303\n",
      "epoch - 69, gen loss - {'gen': 0.3835705816745758}, rmse loss - 7.608639933086409\n",
      "epoch - 70, gen loss - {'gen': 0.35082390904426575}, rmse loss - 7.596478175413124\n",
      "epoch - 71, gen loss - {'gen': 0.41774410009384155}, rmse loss - 7.600349963811051\n",
      "epoch - 72, gen loss - {'gen': 0.38013869524002075}, rmse loss - 7.6091301537967695\n",
      "epoch - 73, gen loss - {'gen': 0.3668568730354309}, rmse loss - 7.601508524145148\n",
      "epoch - 74, gen loss - {'gen': 0.40276607871055603}, rmse loss - 7.5983963914463\n",
      "epoch - 75, gen loss - {'gen': 0.38879460096359253}, rmse loss - 7.635401559931766\n",
      "epoch - 76, gen loss - {'gen': 0.4196012616157532}, rmse loss - 7.60284009454875\n",
      "epoch - 77, gen loss - {'gen': 0.3758925795555115}, rmse loss - 7.615230667195197\n",
      "epoch - 78, gen loss - {'gen': 0.4006074368953705}, rmse loss - 7.612806925914384\n",
      "epoch - 79, gen loss - {'gen': 0.3968299627304077}, rmse loss - 7.6234589735960165\n",
      "epoch - 80, gen loss - {'gen': 0.3744819760322571}, rmse loss - 7.585945708725285\n",
      "epoch - 81, gen loss - {'gen': 0.37637704610824585}, rmse loss - 7.6064721825377966\n",
      "epoch - 82, gen loss - {'gen': 0.37917032837867737}, rmse loss - 7.610925768134339\n",
      "epoch - 83, gen loss - {'gen': 0.3748715817928314}, rmse loss - 7.60892379283905\n",
      "epoch - 84, gen loss - {'gen': 0.36176204681396484}, rmse loss - 7.614450798263409\n",
      "epoch - 85, gen loss - {'gen': 0.3633585274219513}, rmse loss - 7.624955081411834\n",
      "epoch - 86, gen loss - {'gen': 0.3562462031841278}, rmse loss - 7.648597009067606\n",
      "epoch - 87, gen loss - {'gen': 0.3179698586463928}, rmse loss - 7.599279213655479\n",
      "epoch - 88, gen loss - {'gen': 0.3456793427467346}, rmse loss - 7.625342811605587\n",
      "epoch - 89, gen loss - {'gen': 0.3697930574417114}, rmse loss - 7.596264380370559\n",
      "epoch - 90, gen loss - {'gen': 0.37201961874961853}, rmse loss - 7.627905836844357\n",
      "epoch - 91, gen loss - {'gen': 0.3754042983055115}, rmse loss - 7.603596159892769\n",
      "epoch - 92, gen loss - {'gen': 0.37499889731407166}, rmse loss - 7.624911452571404\n",
      "epoch - 93, gen loss - {'gen': 0.3627627491950989}, rmse loss - 7.640920417775087\n",
      "epoch - 94, gen loss - {'gen': 0.36206263303756714}, rmse loss - 7.6021263089127205\n",
      "epoch - 95, gen loss - {'gen': 0.41575485467910767}, rmse loss - 7.6039002821454265\n",
      "epoch - 96, gen loss - {'gen': 0.39114418625831604}, rmse loss - 7.600712839527763\n",
      "epoch - 97, gen loss - {'gen': 0.3907470703125}, rmse loss - 7.607201022415583\n",
      "epoch - 98, gen loss - {'gen': 0.3765701353549957}, rmse loss - 7.624357468527622\n",
      "epoch - 99, gen loss - {'gen': 0.40101879835128784}, rmse loss - 7.613503257286944\n",
      "training end, best epoch - 0, best valid rmse loss - 7.585945708725285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce925b1f4e549de9ef9c69d42cedb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.887 MB of 1.887 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▆▇▃▄▆▇▄▆▄▃▃█▆▅▁▄▇▄▃▅▃▃▂▆▅▇▄▅▂█▇▄▆▆▃▄▅▅▄▆</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂</td></tr><tr><td>Dis_B_step_loss</td><td>▂▁▅▇▆▃▄▄▇▆██▇▅▅▅▇▅▇▄█▇▆▄▅▅▅▆▅▅▇▅▄▅▆▅██▅▅</td></tr><tr><td>Gen_loss</td><td>██▆▅▇▃▃▃▃▄▃▄▅▄▄▃▂▂▂▃▃▃▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00593</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01538</td></tr><tr><td>Dis_B_loss</td><td>-0.0041</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01281</td></tr><tr><td>Gen_loss</td><td>0.39071</td></tr><tr><td>rmse_loss</td><td>7.6135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pious-monkey-24</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2586coef\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2586coef</a><br/>Synced 6 W&B file(s), 300 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221226_101753-2586coef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training(1, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94d456",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #2 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8564c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e996ac",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #3 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dad366",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cb8f4",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #4 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad1d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4698f1",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #1 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9561847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qttqj49) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f8509e66104ff3b1d345f0bf13e6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.626456…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">deep-valley-35</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2qttqj49\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2qttqj49</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_110853-2qttqj49/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qttqj49). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dec71852b04550a5f30b63a0defd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666868527730306, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_110907-9e7sq4w2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/9e7sq4w2\" target=\"_blank\">radiant-water-36</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.7618473768234253}, rmse loss - 14.927160889460152\n",
      "epoch - 1, gen loss - {'gen': 0.6654298305511475}, rmse loss - 14.99635282389792\n",
      "epoch - 2, gen loss - {'gen': 0.5340757369995117}, rmse loss - 14.66809918607733\n",
      "epoch - 3, gen loss - {'gen': 0.4662364423274994}, rmse loss - 15.05842930188478\n",
      "epoch - 4, gen loss - {'gen': 0.391520231962204}, rmse loss - 14.632536296035092\n",
      "epoch - 5, gen loss - {'gen': 0.3538004159927368}, rmse loss - 14.66787469431043\n",
      "epoch - 6, gen loss - {'gen': 0.18090897798538208}, rmse loss - 14.532344453889065\n",
      "epoch - 7, gen loss - {'gen': 0.3389199376106262}, rmse loss - 14.78947099724379\n",
      "epoch - 8, gen loss - {'gen': 0.18239638209342957}, rmse loss - 15.083552317425774\n",
      "epoch - 9, gen loss - {'gen': 0.32673710584640503}, rmse loss - 14.899724779093837\n",
      "epoch - 10, gen loss - {'gen': 0.47688737511634827}, rmse loss - 14.923849262434619\n",
      "epoch - 11, gen loss - {'gen': 0.10778166353702545}, rmse loss - 15.060424085032896\n",
      "epoch - 12, gen loss - {'gen': 0.07433967292308807}, rmse loss - 14.992100434109734\n",
      "epoch - 13, gen loss - {'gen': 0.16034923493862152}, rmse loss - 14.96861992814884\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(1, cfg['epochs'], cfg['device'], 'simtotrain', False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec39b58",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #2 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debee99",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #3 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76075c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3df4c",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #4 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
