{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3534ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 20 17:28:20 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:24:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8     2W / 250W |    965MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:29:00.0 Off |                  N/A |\n",
      "| 38%   63C    P2   102W / 250W |   6421MiB / 11264MiB |     48%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:2E:00.0 Off |                  N/A |\n",
      "| 29%   26C    P8    17W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:33:00.0 Off |                  N/A |\n",
      "| 29%   29C    P8     2W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:60:00.0 Off |                  N/A |\n",
      "| 29%   20C    P8    20W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:6F:00.0 Off |                  N/A |\n",
      "| 29%   21C    P8     6W / 250W |   1531MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:74:00.0 Off |                  N/A |\n",
      "| 29%   24C    P8     1W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  Off  | 00000000:79:00.0 Off |                  N/A |\n",
      "| 29%   23C    P8     8W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      4009      G   /usr/bin/gnome-shell                4MiB |\n",
      "|    0   N/A  N/A   3293991      C   ...nda3/envs/py36/bin/python      947MiB |\n",
      "|    1   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   3293991      C   ...nda3/envs/py36/bin/python     6413MiB |\n",
      "|    2   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A   3466444      C   ...a3/envs/py39_0/bin/python     1523MiB |\n",
      "|    6   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A      3764      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fb727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kji/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_172825-12q3xs3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/12q3xs3f\" target=\"_blank\">olive-frog-7</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/12q3xs3f?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb17dd8dbe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "import cv2, PIL\n",
    "import os, glob\n",
    "import csv, platform\n",
    "\n",
    "current_os = platform.system()\n",
    "if current_os == \"Linux\":\n",
    "    _path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "    cfg = {\n",
    "        'device': \"cuda:5\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 20,\n",
    "        'batch_size': 64,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 4,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "elif current_os == \"Windows\":\n",
    "    _path = 'D:/git_repos/samsung_sem'\n",
    "    cfg = {\n",
    "        'device': \"cuda:0\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 4,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 0,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login(key='0322000365224d30ef0694f60237c68767290052')\n",
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbfc657",
   "metadata": {},
   "source": [
    "# Cycle Gan Model,  Generator:Resnet, Discriminator:PatchGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39fd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(resnet_block, self).__init__()\n",
    "\n",
    "        _resnet_block = [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim)\n",
    "        ]\n",
    "\n",
    "        self.layer = nn.Sequential(*_resnet_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x) + x\n",
    "        return out\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.donw_sampling_layer1 = nn.Sequential(\n",
    "            # down sampling\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.donw_sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.res_block = nn.Sequential(*[resnet_block(64) for i in range(3)])\n",
    "\n",
    "        self.up_samplig_layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.up_samplig_layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(16, input_ch, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.model(x)\n",
    "        out1 = self.init_layer(x)\n",
    "        out2 = self.donw_sampling_layer1(out1)\n",
    "        out3 = self.donw_sampling_layer2(out2)\n",
    "        out4 = self.res_block(out3)\n",
    "        out5 = self.up_samplig_layer1(out4)\n",
    "        out6 = self.up_samplig_layer2(out5)\n",
    "        out7 = self.output_layer(out6)\n",
    "        return out7\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "class PatchGanDiscriminator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(PatchGanDiscriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, stride=1, padding=1, padding_mode='replicate'),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1, bias=True),  # 1\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=True),  # 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'wgan_gp':\n",
    "            self.loss = None\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            if target_is_real:\n",
    "                target_tensor = self.real_label  # .to(self.device)\n",
    "            else:\n",
    "                target_tensor = self.fake_label  # .to(self.device)\n",
    "\n",
    "            target_tensor = target_tensor.expand_as(prediction)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "\n",
    "def _gradient_penalty(netD, real_data, fake_data, type=\"mixed\", constant=1.0, lambda_gp=10.0):\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':  # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1, device=real_data.device)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(\n",
    "                *real_data.shape)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(real_data.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "class cycleGAN_model(nn.Module):\n",
    "    def __init__(self, input_ch=3,\n",
    "                 optim_lr=0.0002,\n",
    "                 gan_mode='lsgan',\n",
    "                 guided=False):\n",
    "        import itertools\n",
    "\n",
    "        super(cycleGAN_model, self).__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.guided = guided\n",
    "\n",
    "        self.Gen = nn.ModuleDict({\n",
    "            'A': ResnetGenerator(input_ch),\n",
    "            'B': ResnetGenerator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Gen['A'], log='all')\n",
    "        # wandb.watch(self.Gen['B'], log='all')\n",
    "\n",
    "        self.Dis = nn.ModuleDict({\n",
    "            'A': PatchGanDiscriminator(input_ch),\n",
    "            'B': PatchGanDiscriminator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Dis['A'], log='all')\n",
    "        # wandb.watch(self.Dis['B'], log='all')\n",
    "\n",
    "        self.optimizer = {\n",
    "            'G': torch.optim.Adam(itertools.chain(self.Gen['A'].parameters(), self.Gen['B'].parameters()), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_A': torch.optim.Adam(self.Dis['A'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_B': torch.optim.Adam(self.Dis['B'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999))\n",
    "        }\n",
    "\n",
    "        self.schedular = {\n",
    "            'G': torch.optim.lr_scheduler.LambdaLR(self.optimizer['G'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_A': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_A'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_B': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_B'], lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "        }\n",
    "\n",
    "        self.criterion = nn.ModuleDict({\n",
    "            'cycle': nn.L1Loss(),\n",
    "            'idt': nn.L1Loss(),\n",
    "            'gan': GANLoss(self.gan_mode),\n",
    "            'mse': nn.MSELoss(),\n",
    "            'guided': nn.L1Loss()\n",
    "        })\n",
    "\n",
    "        self.lambda_idt = 0.5\n",
    "        self.lambda_A = 10.0\n",
    "        self.lambda_B = 10.0\n",
    "\n",
    "    def forward(self, data_A, data_B, mode: str):\n",
    "        if mode == 'gen':\n",
    "            A_out = self.Gen['A'](data_A)\n",
    "            B_out = self.Gen['B'](data_B)\n",
    "        elif mode == 'dis':\n",
    "            A_out = self.Dis['A'](data_A)\n",
    "            B_out = self.Dis['B'](data_B)\n",
    "        else:\n",
    "            raise None\n",
    "        return A_out, B_out\n",
    "\n",
    "    def model_train_discriminator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', True)\n",
    "\n",
    "        self.optimizer['D_B'].zero_grad()\n",
    "\n",
    "        pred_real_B, pred_real_A = self(real_B, real_A, 'dis')  # netA netB\n",
    "        pred_fake_B, pred_fake_A = self(fake_B.detach(), fake_A.detach(), 'dis')\n",
    "\n",
    "        # Discriminator B update\n",
    "        loss_D_B_Real = self.criterion['gan'](pred_real_A, True)\n",
    "        loss_D_B_fake = self.criterion['gan'](pred_fake_A, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_B = (loss_D_B_fake + loss_D_B_Real) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_B = _gradient_penalty(self.Dis['B'], real_A, fake_A.detach())\n",
    "            loss_D_B = loss_D_B_fake + loss_D_B_Real + gradient_penalty_B[0]\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        self.optimizer['D_B'].step()\n",
    "\n",
    "        # Discriminator A update\n",
    "        self.optimizer['D_A'].zero_grad()\n",
    "\n",
    "        loss_D_A_Real = self.criterion['gan'](pred_real_B, True)\n",
    "        loss_D_A_fake = self.criterion['gan'](pred_fake_B, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_A = (loss_D_A_Real + loss_D_A_fake) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_A = _gradient_penalty(self.Dis['A'], real_B, fake_B.detach())\n",
    "            loss_D_A = loss_D_A_Real + loss_D_A_fake + gradient_penalty_A[0]\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        self.optimizer['D_A'].step()\n",
    "\n",
    "        loss_dic = {'dis_a': loss_D_A.item(),\n",
    "                    'dis_b': loss_D_B.item()}\n",
    "\n",
    "        return loss_dic\n",
    "\n",
    "    def model_train_generator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "        rec_B, rec_A = self(fake_A, fake_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', False)\n",
    "        self.optimizer['G'].zero_grad()\n",
    "\n",
    "        idt_A, idt_B = self(real_B, real_A, 'gen')\n",
    "\n",
    "        loss_idt_A = self.criterion['idt'](idt_A, real_B) * self.lambda_B * self.lambda_idt\n",
    "        loss_idt_B = self.criterion['idt'](idt_B, real_A) * self.lambda_A * self.lambda_idt\n",
    "\n",
    "        dis_A_fake_B, dis_B_fake_A = self(fake_B, fake_A, 'dis')  # dis_A(fake_B) / dis_B(fake_A)\n",
    "\n",
    "        loss_G_A = self.criterion['gan'](dis_A_fake_B, True)\n",
    "        loss_G_B = self.criterion['gan'](dis_B_fake_A, True)\n",
    "\n",
    "        loss_cycle_A = self.criterion['cycle'](rec_A, real_A) * self.lambda_A\n",
    "        loss_cycle_B = self.criterion['cycle'](rec_B, real_B) * self.lambda_B\n",
    "\n",
    "        # Guied Loss (paired)\n",
    "        if self.guided:\n",
    "            loss_guided_A = self.criterion['guided'](fake_B, real_B)\n",
    "            loss_guided_B = self.criterion['guided'](fake_A, real_A)\n",
    "        else:\n",
    "            loss_guided_A = 0\n",
    "            loss_guided_B = 0\n",
    "        ##########\n",
    "\n",
    "        loss_Gen = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B + loss_guided_A + loss_guided_B\n",
    "        loss_Gen.backward()\n",
    "\n",
    "        self.optimizer['G'].step()\n",
    "\n",
    "        loss_dic = {'gen': loss_Gen.item()}\n",
    "\n",
    "        inference_image = {\n",
    "            'real_a': real_A,\n",
    "            'real_b': real_B,\n",
    "            'atob_fake': fake_B,\n",
    "            'btoa_fake': fake_A,\n",
    "            'rec_a': rec_A,\n",
    "            'rec_b': rec_B\n",
    "        }\n",
    "\n",
    "        return loss_dic, {key: self.tensortonp(inference_image[key]) for key in inference_image}\n",
    "\n",
    "    def model_valid(self, real_A, real_B):\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "            true = (real_B * 255).type(torch.uint8).float()\n",
    "            fake_true = (fake_B * 255).type(torch.uint8).float()\n",
    "            rmse_loss = torch.sqrt(self.criterion['mse'](fake_true, true))\n",
    "\n",
    "        img_dict = {\n",
    "            'real_A': real_A,\n",
    "            'fake_B': fake_B,\n",
    "\n",
    "            'real_B': real_B,\n",
    "            'fake_A': fake_A,\n",
    "        }\n",
    "\n",
    "        return rmse_loss.item(), {key: self.tensortonp(img_dict[key]) for key in img_dict}\n",
    "\n",
    "    def tensortonp(self, tensor):\n",
    "        return (tensor.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    def set_requires_grad(self, net_type='dis', mode=True):\n",
    "        if net_type == 'gen':\n",
    "            net_dic = self.Gen\n",
    "        elif net_type == 'dis':\n",
    "            net_dic = self.Dis\n",
    "\n",
    "        for key in net_dic:\n",
    "            net_dic[key].set_requires_grad(mode)\n",
    "\n",
    "    def schedular_step(self):\n",
    "        self.schedular['G'].step()\n",
    "        self.schedular['D_A'].step()\n",
    "        self.schedular['D_B'].step()\n",
    "\n",
    "    def model_save(self, PATH):\n",
    "        temp_dict = {}\n",
    "        key_list = [key for key in self.__dict__.keys() if not '_' in key[0]]\n",
    "        key_list.extend([key for key in self.__dict__['_modules'].keys()])\n",
    "\n",
    "        for key in key_list:\n",
    "            if hasattr(self, key):\n",
    "                value = getattr(self, key)\n",
    "                if isinstance(value, dict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = {}\n",
    "                    for sub_key in value.keys():\n",
    "                        if not sub_key in temp_dict[key]:\n",
    "                            temp_dict[key][sub_key] = value[sub_key].state_dict()\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value.state_dict()\n",
    "                else:\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value\n",
    "\n",
    "        torch.save(temp_dict, PATH)\n",
    "\n",
    "    def model_load(self, PATH, device):\n",
    "        state_dict = torch.load(PATH, map_location=device)\n",
    "\n",
    "        for cls_key in state_dict.keys():\n",
    "            if hasattr(self, cls_key):\n",
    "                value = getattr(self, cls_key)\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key in value.keys():\n",
    "                        value[sub_key].load_state_dict(state_dict[cls_key][sub_key])\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    value.load_state_dict(state_dict[cls_key])\n",
    "                else:\n",
    "                    setattr(self, cls_key, state_dict[cls_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf110b",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4d0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list(abs_path):\n",
    "    # abs_path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "\n",
    "    # Dataset path\n",
    "    sim_depth_path = os.path.join(abs_path, 'simulation_data/Depth')\n",
    "    sim_sem_path = os.path.join(abs_path, 'simulation_data/SEM')\n",
    "\n",
    "    train_path = os.path.join(abs_path, 'train')\n",
    "\n",
    "    # only Test\n",
    "    test_path = os.path.join(abs_path, 'test/SEM')\n",
    "\n",
    "    sim_depth_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_depth_path):\n",
    "        if not case in sim_depth_img_path_dic:\n",
    "            sim_depth_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_depth_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_depth_path, case, folder, '*.png'))\n",
    "            for img in img_list:\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "\n",
    "    sim_sem_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_sem_path):\n",
    "        if not case in sim_sem_img_path_dic:\n",
    "            sim_sem_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_sem_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_sem_path, case, folder, '*.png'))\n",
    "            sim_sem_img_path_dic[case].extend(img_list)\n",
    "\n",
    "    train_avg_depth = dict()\n",
    "    with open(os.path.join(train_path, \"average_depth.csv\"), 'r') as csvfile:\n",
    "        temp = csv.reader(csvfile)\n",
    "        for idx, line in enumerate(temp):\n",
    "            if idx > 0:\n",
    "                depth_key, site_key = line[0].split('_site')\n",
    "                depth_key = depth_key.replace(\"d\", \"D\")\n",
    "                site_key = \"site\" + site_key\n",
    "                if not depth_key in train_avg_depth:\n",
    "                    train_avg_depth[depth_key] = dict()\n",
    "\n",
    "                train_avg_depth[depth_key][site_key] = float(line[1])\n",
    "\n",
    "    train_img_path_dic = dict()\n",
    "    for depth in os.listdir(os.path.join(train_path, \"SEM\")):\n",
    "        if not depth in train_img_path_dic:\n",
    "            train_img_path_dic[depth] = []\n",
    "        for site in os.listdir(os.path.join(train_path, \"SEM\", depth)):\n",
    "            img_list = glob.glob(os.path.join(train_path, \"SEM\", depth, site, \"*.png\"))\n",
    "            train_img_path_dic[depth].extend([[temp_img, train_avg_depth[depth][site]] for temp_img in img_list])\n",
    "\n",
    "    test_img_path_list = glob.glob(os.path.join(test_path, \"*.png\"))\n",
    "\n",
    "    result_dic = dict()\n",
    "    result_dic['sim'] = dict()\n",
    "    result_dic['sim']['sem'] = sim_sem_img_path_dic\n",
    "    result_dic['sim']['depth'] = sim_depth_img_path_dic\n",
    "    result_dic['train'] = train_img_path_dic\n",
    "    result_dic['test'] = np.array(test_img_path_list)\n",
    "    result_dic['train_avg_depth'] = train_avg_depth\n",
    "\n",
    "    return result_dic\n",
    "\n",
    "result_dic = get_img_list(cfg['db_path'])\n",
    "\n",
    "'''\n",
    "train sem ->  sim sem -> sim depth\n",
    "\n",
    "case 별로 dataset을 나눠야됨.\n",
    "'''\n",
    "\n",
    "class gan_dataset(Dataset):\n",
    "    def __init__(self, a_data_path, b_data_path, transform=None):\n",
    "        super(gan_dataset, self).__init__()\n",
    "        self.a_data_path = a_data_path\n",
    "        self.b_data_path = b_data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.a_size = len(a_data_path)\n",
    "        self.b_size = len(b_data_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.a_size > self.b_size:\n",
    "            a_idx = idx\n",
    "            b_idx = idx % self.b_size\n",
    "        else:\n",
    "            a_idx = idx % self.a_size\n",
    "            b_idx = idx\n",
    "        if isinstance(self.a_data_path[a_idx], str):\n",
    "            a_path = self.a_data_path[a_idx]\n",
    "        elif isinstance(self.a_data_path[a_idx], list):\n",
    "            a_path = self.a_data_path[a_idx][0]\n",
    "\n",
    "        if isinstance(self.b_data_path[b_idx], str):\n",
    "            b_path = self.b_data_path[b_idx]\n",
    "        elif isinstance(self.b_data_path[b_idx], list):\n",
    "            b_path = self.b_data_path[b_idx][0]\n",
    "\n",
    "        a_img = PIL.Image.open(a_path).convert(\"L\")\n",
    "        b_img = PIL.Image.open(b_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            a_img = self.transform(a_img)\n",
    "            b_img = self.transform(b_img)\n",
    "\n",
    "        a_img = (np.array(a_img) / 255.)\n",
    "        a_img = a_img.reshape(1, *a_img.shape).astype(np.float32)\n",
    "        b_img = (np.array(b_img) / 255.)\n",
    "        b_img = b_img.reshape(1, *b_img.shape).astype(np.float32)\n",
    "\n",
    "        return a_img, b_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.a_data_path), len(self.b_data_path))\n",
    "\n",
    "def create_dataloader(a_key, b_key, t_ratio, result_dic, case=1):\n",
    "    if 'sim' in a_key:\n",
    "        a_list = result_dic['sim'][a_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        a_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "    if 'sim' in b_key:\n",
    "        b_list = result_dic['sim'][b_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        b_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "\n",
    "    horizon_transform = transforms.RandomHorizontalFlip(1.0)\n",
    "    rotate_transform = transforms.RandomRotation((180, 180))\n",
    "    vertical_transform = transforms.RandomVerticalFlip(1.0)\n",
    "\n",
    "    a_train_data_size = int(len(a_list) * t_ratio)\n",
    "    b_train_data_size = int(len(b_list) * t_ratio)\n",
    "\n",
    "    train_dataset = gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], None) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], vertical_transform)\n",
    "\n",
    "    valid_dataset = gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], None) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], vertical_transform)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=True), \\\n",
    "           DataLoader(valid_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdec56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataloader, device):\n",
    "    rmse_list = []\n",
    "    for step_i, data_tuple in enumerate(valid_dataloader):\n",
    "        real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "        real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "        rmse_loss, img_dict = model.model_valid(real_a, real_b)\n",
    "        rmse_list.append(rmse_loss)\n",
    "        if step_i == 0:\n",
    "            img_list = [img_dict[key][0][0] for key in img_dict]\n",
    "            img_list = [wandb.Image(PIL.Image.fromarray(np.concatenate((img_list[i], img_list[i+1]), axis=-1)).convert('L'), caption=key)\n",
    "                        for i, key in enumerate(img_dict.keys()) if i % 2 == 0]\n",
    "            wandb.log({\n",
    "                \"example image\": img_list\n",
    "            })\n",
    "            \n",
    "\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "def training(case, epochs, device, type, checkpoint_path=None):\n",
    "    best_rmse_loss = 9999\n",
    "    critic_iter = 5\n",
    "    best_epoch = 0\n",
    "\n",
    "    if type == 'semtodepth':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'sim_depth'\n",
    "    elif type == 'simtotrain':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'train'\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloader(a_key=a_key,\n",
    "                                                           b_key=b_key,\n",
    "                                                           t_ratio=0.8,\n",
    "                                                           result_dic=result_dic,\n",
    "                                                           case=case)\n",
    "\n",
    "    model = cycleGAN_model(1, optim_lr=0.0002, gan_mode='wgan_gp', guided=False)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        model.model_load(checkpoint_path, device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_list = [[], [], []]\n",
    "        for step_i, data_tuple in enumerate(train_dataloader):\n",
    "            real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "            real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "            dis_loss = model.model_train_discriminator(real_a, real_b)\n",
    "            loss_list[1].append(dis_loss['dis_a'])\n",
    "            loss_list[2].append(dis_loss['dis_b'])\n",
    "            if step_i % critic_iter == 0:\n",
    "                gen_loss, img_dic = model.model_train_generator(real_a, real_b)\n",
    "                loss_list[0].append(gen_loss['gen'])\n",
    "\n",
    "                wandb.log({\n",
    "                    'Gen_step_loss': gen_loss,\n",
    "                    'Dis_A_step_loss': dis_loss['dis_a'],\n",
    "                    'Dis_B_step_loss': dis_loss['dis_b']\n",
    "                })\n",
    "\n",
    "        rmse_loss = valid(model, valid_dataloader, device)\n",
    "        print(f'epoch - {epoch}, gen loss - {gen_loss}, rmse loss - {rmse_loss}')\n",
    "        wandb.log({\n",
    "            'Gen_loss': np.mean(loss_list[0]),\n",
    "            'Dis_A_loss': np.mean(loss_list[1]),\n",
    "            'Dis_B_loss': np.mean(loss_list[2]),\n",
    "            'learning_rate': model.schedular['G'].get_lr(),\n",
    "            'rmse_loss': rmse_loss\n",
    "        })\n",
    "\n",
    "        if best_rmse_loss > rmse_loss:\n",
    "            best_rmse_loss = rmse_loss\n",
    "            model.model_save(f'./case{case}_t({type})_best_model.pth')\n",
    "\n",
    "        model.schedular_step()\n",
    "    print(f'training end, best epoch - {best_epoch}, best valid rmse loss - {best_rmse_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08579cc2",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f908c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.8828357458114624}, rmse loss - 14.070308819028284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kji/anaconda3/envs/py39_0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 1, gen loss - {'gen': 0.732869565486908}, rmse loss - 11.34548017459602\n",
      "epoch - 2, gen loss - {'gen': 0.637847363948822}, rmse loss - 10.710058736625193\n",
      "epoch - 3, gen loss - {'gen': 0.6217143535614014}, rmse loss - 10.229482085942342\n",
      "epoch - 4, gen loss - {'gen': 0.6896553635597229}, rmse loss - 10.382899103129482\n",
      "epoch - 5, gen loss - {'gen': 0.8414745926856995}, rmse loss - 9.813263956471124\n",
      "epoch - 6, gen loss - {'gen': 0.8875612020492554}, rmse loss - 10.42074014691849\n",
      "epoch - 7, gen loss - {'gen': 0.9963855743408203}, rmse loss - 9.485837547541545\n",
      "epoch - 8, gen loss - {'gen': 1.0545477867126465}, rmse loss - 9.414364807720114\n",
      "epoch - 9, gen loss - {'gen': 0.9227295517921448}, rmse loss - 9.216501322179703\n",
      "epoch - 10, gen loss - {'gen': 0.7758920788764954}, rmse loss - 9.243293517630038\n",
      "epoch - 11, gen loss - {'gen': 0.8474010229110718}, rmse loss - 8.970940902224326\n",
      "epoch - 12, gen loss - {'gen': 0.9224570393562317}, rmse loss - 8.926743596242362\n",
      "epoch - 13, gen loss - {'gen': 0.8617584109306335}, rmse loss - 8.89842577174141\n",
      "epoch - 14, gen loss - {'gen': 0.7776334285736084}, rmse loss - 8.795363113888955\n",
      "epoch - 15, gen loss - {'gen': 0.754216730594635}, rmse loss - 8.874397869919498\n",
      "epoch - 16, gen loss - {'gen': 0.9367407560348511}, rmse loss - 8.785595458372052\n",
      "epoch - 17, gen loss - {'gen': 0.8577091693878174}, rmse loss - 8.781968251365576\n",
      "epoch - 18, gen loss - {'gen': 0.8363178372383118}, rmse loss - 8.675802772775347\n",
      "epoch - 19, gen loss - {'gen': 0.9009798765182495}, rmse loss - 8.669748195422972\n",
      "training end, best epoch - 0, best valid rmse loss - 8.669748195422972\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▄▄▃▄▃▃▃▄▂▃▄▄▄▄▇▄▅▃▅▂▁▃▃▄▄▄▃▂▃▂▂▄▂▄▂▅▄▂▃</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▇▇▅▅▅▅▄▅▄▂▂▃▃▁▅▃▂▃▄▂▄▃▅▅▅▄▄▄▃▅▄▅▆▆▅▅▅▅▇</td></tr><tr><td>Gen_loss</td><td>█▃▁▁▂▂▄▃▅▄▃▂▃▃▂▂▃▃▃▃</td></tr><tr><td>rmse_loss</td><td>█▄▄▃▃▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00463</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01174</td></tr><tr><td>Dis_B_loss</td><td>-0.02047</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01562</td></tr><tr><td>Gen_loss</td><td>0.80213</td></tr><tr><td>rmse_loss</td><td>8.66975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">olive-frog-7</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/12q3xs3f\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/12q3xs3f</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_172825-12q3xs3f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training(1, cfg['epochs'], cfg['device'], 'semtodepth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64162955",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848c46d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4ec41e2ff2479f8babedc8fe3089f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0166687672957778, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_183503-2ey6tqne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2ey6tqne\" target=\"_blank\">devout-eon-8</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.6873235702514648}, rmse loss - 12.654050855179115\n",
      "epoch - 1, gen loss - {'gen': 0.5774610042572021}, rmse loss - 11.361319886802308\n",
      "epoch - 2, gen loss - {'gen': 0.567801833152771}, rmse loss - 10.983229725123332\n",
      "epoch - 3, gen loss - {'gen': 0.37087684869766235}, rmse loss - 10.844309734682316\n",
      "epoch - 4, gen loss - {'gen': 0.4872058033943176}, rmse loss - 10.369859051440475\n",
      "epoch - 5, gen loss - {'gen': 0.4414686858654022}, rmse loss - 10.110518188054272\n",
      "epoch - 6, gen loss - {'gen': 0.4029061794281006}, rmse loss - 9.815545189424634\n",
      "epoch - 7, gen loss - {'gen': 0.6086832880973816}, rmse loss - 9.740016854557163\n",
      "epoch - 8, gen loss - {'gen': 0.5154827833175659}, rmse loss - 9.544583519446453\n",
      "epoch - 9, gen loss - {'gen': 0.4885183870792389}, rmse loss - 9.477411025564608\n",
      "epoch - 10, gen loss - {'gen': 0.5194069743156433}, rmse loss - 9.280928652224945\n",
      "epoch - 11, gen loss - {'gen': 0.578315258026123}, rmse loss - 9.267966506226037\n",
      "epoch - 12, gen loss - {'gen': 0.48055052757263184}, rmse loss - 9.167899400985549\n",
      "epoch - 13, gen loss - {'gen': 0.500241756439209}, rmse loss - 9.268083642769565\n",
      "epoch - 14, gen loss - {'gen': 0.4400656819343567}, rmse loss - 9.109165463500357\n",
      "epoch - 15, gen loss - {'gen': 0.43955421447753906}, rmse loss - 9.078652429404734\n",
      "epoch - 16, gen loss - {'gen': 0.4330228567123413}, rmse loss - 9.105411111648673\n",
      "epoch - 17, gen loss - {'gen': 0.37273672223091125}, rmse loss - 9.089110750113907\n",
      "epoch - 18, gen loss - {'gen': 0.36033326387405396}, rmse loss - 9.075684726018308\n",
      "epoch - 19, gen loss - {'gen': 0.38270121812820435}, rmse loss - 8.968388301420037\n",
      "training end, best epoch - 0, best valid rmse loss - 8.968388301420037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337eef9526944b35b691e3d5c2cb7759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.152 MB of 0.152 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▄▅▅▄▅▄▅▄▃▄▆▁▂▄▄▄▃▄▄▆▄▄▆▆▅▆▃▅▅▅▆▆▅▄▅▆▄▄▆</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▆▇▇█▆▆▅▂▅▁▂▃▃▄▅▆▆▅▅▅▆▇▆▇▆▆▆▆▇▇▆▆▇█▆▆▇▆█</td></tr><tr><td>Gen_loss</td><td>█▃▃▂▁▂▂▂▃▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>rmse_loss</td><td>█▆▅▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00511</td></tr><tr><td>Dis_A_step_loss</td><td>-0.00784</td></tr><tr><td>Dis_B_loss</td><td>-0.00611</td></tr><tr><td>Dis_B_step_loss</td><td>0.00206</td></tr><tr><td>Gen_loss</td><td>0.46423</td></tr><tr><td>rmse_loss</td><td>8.96839</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">devout-eon-8</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2ey6tqne\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/2ey6tqne</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_183503-2ey6tqne/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'semtodepth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02672a1f",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #3 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb4118f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51dfcb4ab248498f0f03a4fba5ab92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668306570500134, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_194052-ily5yyxy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/ily5yyxy\" target=\"_blank\">electric-bird-9</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.7505766153335571}, rmse loss - 12.199230011098939\n",
      "epoch - 1, gen loss - {'gen': 0.7420825958251953}, rmse loss - 12.049418759082076\n",
      "epoch - 2, gen loss - {'gen': 0.7387523651123047}, rmse loss - 11.474365355783723\n",
      "epoch - 3, gen loss - {'gen': 0.8143324255943298}, rmse loss - 10.449388935117264\n",
      "epoch - 4, gen loss - {'gen': 0.6743029952049255}, rmse loss - 10.80027136151641\n",
      "epoch - 5, gen loss - {'gen': 0.699018120765686}, rmse loss - 10.41661036410455\n",
      "epoch - 6, gen loss - {'gen': 0.41321662068367004}, rmse loss - 10.449165866823654\n",
      "epoch - 7, gen loss - {'gen': 0.7177731990814209}, rmse loss - 9.987706061218937\n",
      "epoch - 8, gen loss - {'gen': 0.8271799087524414}, rmse loss - 9.858945473533716\n",
      "epoch - 9, gen loss - {'gen': 0.7736988663673401}, rmse loss - 9.959403159433625\n",
      "epoch - 10, gen loss - {'gen': 0.6344614028930664}, rmse loss - 9.523836873114329\n",
      "epoch - 11, gen loss - {'gen': 0.7110041379928589}, rmse loss - 9.410774009254146\n",
      "epoch - 12, gen loss - {'gen': 0.6927868723869324}, rmse loss - 9.646608384332973\n",
      "epoch - 13, gen loss - {'gen': 0.6216289401054382}, rmse loss - 9.808001358130761\n",
      "epoch - 14, gen loss - {'gen': 0.6848387122154236}, rmse loss - 9.347014085832997\n",
      "epoch - 15, gen loss - {'gen': 0.591212809085846}, rmse loss - 9.625839474456336\n",
      "epoch - 16, gen loss - {'gen': 0.5353155732154846}, rmse loss - 9.808472853305155\n",
      "epoch - 17, gen loss - {'gen': 0.6474960446357727}, rmse loss - 9.426055677702506\n",
      "epoch - 18, gen loss - {'gen': 0.7299900054931641}, rmse loss - 9.52405789329557\n",
      "epoch - 19, gen loss - {'gen': 0.543077826499939}, rmse loss - 9.721678311534474\n",
      "training end, best epoch - 0, best valid rmse loss - 9.347014085832997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed114d5260d4c7ba1230478c4c9f391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.153 MB of 0.153 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▃▅▄▃▄▃▅▃▆▁▅▃▇▃▇▄▆▆▂▆▄▄▂▃▄█▄▅▄▂▄▄▁▃▃▅▂▃▄▄</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▆▅▇▆▆▄▄▄▂▄▃▁▂▃▅▄▅▆▆▅▄▄▇▄▅▆▅▅▅▇▅▄▆▆▄▆▄▅▆</td></tr><tr><td>Gen_loss</td><td>█▃▃▃▃▂▃▂▃▄▃▃▂▂▂▂▁▁▂▁</td></tr><tr><td>rmse_loss</td><td>██▆▄▅▄▄▃▂▃▁▁▂▂▁▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00347</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01493</td></tr><tr><td>Dis_B_loss</td><td>-0.00533</td></tr><tr><td>Dis_B_step_loss</td><td>-0.00692</td></tr><tr><td>Gen_loss</td><td>0.57833</td></tr><tr><td>rmse_loss</td><td>9.72168</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-bird-9</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/ily5yyxy\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/ily5yyxy</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_194052-ily5yyxy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'semtodepth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c202bd",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f87dc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b378d2633eec439eb2073ba800fb151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668449528515338, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_204632-33ei3to8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/33ei3to8\" target=\"_blank\">noble-snowball-10</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.7997966408729553}, rmse loss - 14.39755693660891\n",
      "epoch - 1, gen loss - {'gen': 0.7898048162460327}, rmse loss - 13.422149820081422\n",
      "epoch - 2, gen loss - {'gen': 0.8027539253234863}, rmse loss - 11.851388190505249\n",
      "epoch - 3, gen loss - {'gen': 0.8752145767211914}, rmse loss - 11.99314035964628\n",
      "epoch - 4, gen loss - {'gen': 0.9320234656333923}, rmse loss - 11.719426950405445\n",
      "epoch - 5, gen loss - {'gen': 0.6710600852966309}, rmse loss - 11.256876021733584\n",
      "epoch - 6, gen loss - {'gen': 0.8080239295959473}, rmse loss - 11.699465841384832\n",
      "epoch - 7, gen loss - {'gen': 0.5610429644584656}, rmse loss - 11.102587485225438\n",
      "epoch - 8, gen loss - {'gen': 0.7912049293518066}, rmse loss - 11.087332165989048\n",
      "epoch - 9, gen loss - {'gen': 0.8380870223045349}, rmse loss - 11.44719719974757\n",
      "epoch - 10, gen loss - {'gen': 0.6010779142379761}, rmse loss - 10.6982882506733\n",
      "epoch - 11, gen loss - {'gen': 0.6761199235916138}, rmse loss - 11.267386128541728\n",
      "epoch - 12, gen loss - {'gen': 0.655735194683075}, rmse loss - 10.954258871254446\n",
      "epoch - 13, gen loss - {'gen': 0.7163793444633484}, rmse loss - 11.240164179643582\n",
      "epoch - 14, gen loss - {'gen': 0.6972745060920715}, rmse loss - 10.778849849841691\n",
      "epoch - 15, gen loss - {'gen': 0.8064659833908081}, rmse loss - 11.115364771487528\n",
      "epoch - 16, gen loss - {'gen': 0.6653453707695007}, rmse loss - 11.211863797529157\n",
      "epoch - 17, gen loss - {'gen': 0.7120242118835449}, rmse loss - 11.274648157872837\n",
      "epoch - 18, gen loss - {'gen': 0.8245687484741211}, rmse loss - 11.261898501772722\n",
      "epoch - 19, gen loss - {'gen': 0.6878788471221924}, rmse loss - 11.003534276547027\n",
      "training end, best epoch - 0, best valid rmse loss - 10.6982882506733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a327ab62d1346cd89e89b78610e26e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.153 MB of 0.153 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▅▃▅▂▄▆▄▄▃█▃▃▄▃▄▄▃▅▁▆▃▃▅▇▆▃▃▃▆▄▂▄▂▇▄▃▃▆▅▃</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▇▇▆▆▆▅▅▅▄▄▂▁▁▂▃▆▄▆▆▆▇█▇▆▇▆▆▇▆▇▇▆▆▇▇█▇▆▇</td></tr><tr><td>Gen_loss</td><td>█▃▂▄▄▃▂▁▁▃▂▁▂▂▂▂▂▂▃▃</td></tr><tr><td>rmse_loss</td><td>█▆▃▃▃▂▃▂▂▂▁▂▁▂▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00379</td></tr><tr><td>Dis_A_step_loss</td><td>-0.00989</td></tr><tr><td>Dis_B_loss</td><td>-0.00401</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01211</td></tr><tr><td>Gen_loss</td><td>0.80726</td></tr><tr><td>rmse_loss</td><td>11.00353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">noble-snowball-10</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/33ei3to8\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/33ei3to8</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_204632-33ei3to8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'semtodepth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139f1ea",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c4358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445d6e56db0841dc999638094f9a26e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666818152492245, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_215201-jwrmpubj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/jwrmpubj\" target=\"_blank\">clean-vortex-11</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.8315116167068481}, rmse loss - 15.241546637897562\n",
      "epoch - 1, gen loss - {'gen': 0.886871337890625}, rmse loss - 15.313204161795303\n",
      "epoch - 2, gen loss - {'gen': 0.9445165395736694}, rmse loss - 14.793669556339728\n",
      "epoch - 3, gen loss - {'gen': 0.7500483393669128}, rmse loss - 14.86934026316963\n",
      "epoch - 4, gen loss - {'gen': 0.7464839220046997}, rmse loss - 14.687257076981323\n",
      "epoch - 5, gen loss - {'gen': 0.8343082666397095}, rmse loss - 15.15398197244454\n",
      "epoch - 6, gen loss - {'gen': 0.6902083158493042}, rmse loss - 14.884207503822017\n",
      "epoch - 7, gen loss - {'gen': 0.8129852414131165}, rmse loss - 14.605241877566405\n",
      "epoch - 8, gen loss - {'gen': 0.6748891472816467}, rmse loss - 14.67824569047597\n",
      "epoch - 9, gen loss - {'gen': 0.6336427330970764}, rmse loss - 14.473432030625009\n",
      "epoch - 10, gen loss - {'gen': 0.5238527059555054}, rmse loss - 14.619664976957539\n",
      "epoch - 11, gen loss - {'gen': 0.5008187294006348}, rmse loss - 14.851057847927418\n",
      "epoch - 12, gen loss - {'gen': 0.5763381719589233}, rmse loss - 14.827810977217895\n",
      "epoch - 13, gen loss - {'gen': 0.589530885219574}, rmse loss - 15.303086968805518\n",
      "epoch - 14, gen loss - {'gen': 0.41411036252975464}, rmse loss - 14.823923988975721\n",
      "epoch - 15, gen loss - {'gen': 0.4278516173362732}, rmse loss - 14.948136716751154\n",
      "epoch - 16, gen loss - {'gen': 0.405364453792572}, rmse loss - 15.202469102570932\n",
      "epoch - 17, gen loss - {'gen': 0.4782200753688812}, rmse loss - 15.252714605788903\n",
      "epoch - 18, gen loss - {'gen': 0.40640127658843994}, rmse loss - 15.15211176256412\n",
      "epoch - 19, gen loss - {'gen': 0.3997846841812134}, rmse loss - 15.082239594406747\n",
      "training end, best epoch - 0, best valid rmse loss - 14.473432030625009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b227e9f1a5473d81de7cd939d30098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.183 MB of 0.183 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▆▅██▅▄█▄▅▆▄▃▅▄▃▂▄▃▁▂▅▆▂▄▃▂▁▄▄▄▃▄▅▃▃▂▁▄▃▃</td></tr><tr><td>Dis_B_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>Dis_B_step_loss</td><td>▇▇▇▇▅█▄▃▂▂▃▄▂▂█▂▁▂▃▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▆▃▄▅▅▆</td></tr><tr><td>Gen_loss</td><td>█▅▅▅▄▅▄▃▃▂▂▂▂▂▁▂▁▁▂▁</td></tr><tr><td>rmse_loss</td><td>▇█▄▄▃▇▄▂▃▁▂▄▄█▄▅▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00788</td></tr><tr><td>Dis_A_step_loss</td><td>-0.00286</td></tr><tr><td>Dis_B_loss</td><td>-0.00985</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01444</td></tr><tr><td>Gen_loss</td><td>0.39827</td></tr><tr><td>rmse_loss</td><td>15.08224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clean-vortex-11</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/jwrmpubj\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/jwrmpubj</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_215201-jwrmpubj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(1, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c097bdb",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddef956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9e0de6bd3d480b852c7d4323202f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668400509903827, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221220_225850-spvynzen</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/spvynzen\" target=\"_blank\">woven-snowflake-12</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.8961015939712524}, rmse loss - 16.59954828177871\n",
      "epoch - 1, gen loss - {'gen': 0.8622493743896484}, rmse loss - 16.217989654118725\n",
      "epoch - 2, gen loss - {'gen': 0.7488946914672852}, rmse loss - 15.850355790549978\n",
      "epoch - 3, gen loss - {'gen': 0.6583397388458252}, rmse loss - 16.152184109846164\n",
      "epoch - 4, gen loss - {'gen': 0.35627281665802}, rmse loss - 15.115385080175646\n",
      "epoch - 5, gen loss - {'gen': 0.4775915741920471}, rmse loss - 15.221524557064381\n",
      "epoch - 6, gen loss - {'gen': 0.45574578642845154}, rmse loss - 14.714787678525017\n",
      "epoch - 7, gen loss - {'gen': 0.4439077377319336}, rmse loss - 14.164325184487769\n",
      "epoch - 8, gen loss - {'gen': 0.46058571338653564}, rmse loss - 14.127571596870563\n",
      "epoch - 9, gen loss - {'gen': 0.5578083992004395}, rmse loss - 13.897596295909247\n",
      "epoch - 10, gen loss - {'gen': 0.4000035226345062}, rmse loss - 13.68375829049142\n",
      "epoch - 11, gen loss - {'gen': 0.45067864656448364}, rmse loss - 13.579058135127669\n",
      "epoch - 12, gen loss - {'gen': 0.3626900017261505}, rmse loss - 13.50701880718949\n",
      "epoch - 13, gen loss - {'gen': 0.45324862003326416}, rmse loss - 13.766234429560024\n",
      "epoch - 14, gen loss - {'gen': 0.4089283347129822}, rmse loss - 13.85105067601503\n",
      "epoch - 15, gen loss - {'gen': 0.34234553575515747}, rmse loss - 13.737347856218964\n",
      "epoch - 16, gen loss - {'gen': 0.32374054193496704}, rmse loss - 13.823872705227334\n",
      "epoch - 17, gen loss - {'gen': 0.29661715030670166}, rmse loss - 13.730604456799497\n",
      "epoch - 18, gen loss - {'gen': 0.3724433183670044}, rmse loss - 13.81508346325357\n",
      "epoch - 19, gen loss - {'gen': 0.358551561832428}, rmse loss - 13.84701240546589\n",
      "training end, best epoch - 0, best valid rmse loss - 13.50701880718949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb7727a2b86422aa458f0a097105eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.182 MB of 0.182 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▄▄▆█▄▃▅▃▃▅▂▃▃▂▄▄▅█▃▇▆▆▃▆▇▃▂▃▃▆▄▅▄▄▆▇▃▅▃▁</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▆▆▆▅▄▄▄▄▄▂▄▁▁▂▃▃▄▄▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▆▆▅▇▅▅</td></tr><tr><td>Gen_loss</td><td>█▅▅▄▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▇▆▇▅▅▄▂▂▂▁▁▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00649</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01708</td></tr><tr><td>Dis_B_loss</td><td>-0.00834</td></tr><tr><td>Dis_B_step_loss</td><td>-0.00535</td></tr><tr><td>Gen_loss</td><td>0.32646</td></tr><tr><td>rmse_loss</td><td>13.84701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-snowflake-12</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/spvynzen\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/spvynzen</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_225850-spvynzen/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3471c5",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #3 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd94cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50f3dfb0f11470ea8b6c9f6098bdd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668320012589295, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221221_000634-e3vwcagu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/e3vwcagu\" target=\"_blank\">blooming-snowball-13</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.9641523957252502}, rmse loss - 16.735494219508997\n",
      "epoch - 1, gen loss - {'gen': 0.9151111841201782}, rmse loss - 16.503922133428144\n",
      "epoch - 2, gen loss - {'gen': 0.7946575880050659}, rmse loss - 16.474753738769305\n",
      "epoch - 3, gen loss - {'gen': 0.6876673698425293}, rmse loss - 15.7485258253738\n",
      "epoch - 4, gen loss - {'gen': 0.43963876366615295}, rmse loss - 15.821471974418612\n",
      "epoch - 5, gen loss - {'gen': 0.6035048961639404}, rmse loss - 15.349621359272637\n",
      "epoch - 6, gen loss - {'gen': 0.48806431889533997}, rmse loss - 15.328797731012436\n",
      "epoch - 7, gen loss - {'gen': 0.35913604497909546}, rmse loss - 14.862865148875107\n",
      "epoch - 8, gen loss - {'gen': 0.42915278673171997}, rmse loss - 14.904281334683464\n",
      "epoch - 9, gen loss - {'gen': 0.4557139277458191}, rmse loss - 14.551125948719433\n",
      "epoch - 10, gen loss - {'gen': 0.47903814911842346}, rmse loss - 14.528772313656402\n",
      "epoch - 11, gen loss - {'gen': 0.4311296045780182}, rmse loss - 14.386502966229767\n",
      "epoch - 12, gen loss - {'gen': 0.4480474591255188}, rmse loss - 14.446861984984901\n",
      "epoch - 13, gen loss - {'gen': 0.508722186088562}, rmse loss - 14.535735355532037\n",
      "epoch - 14, gen loss - {'gen': 0.5442911982536316}, rmse loss - 14.622381915905379\n",
      "epoch - 15, gen loss - {'gen': 0.31679004430770874}, rmse loss - 14.399228938831175\n",
      "epoch - 16, gen loss - {'gen': 0.3994178771972656}, rmse loss - 14.467154050665147\n",
      "epoch - 17, gen loss - {'gen': 0.40010493993759155}, rmse loss - 14.525140769367288\n",
      "epoch - 18, gen loss - {'gen': 0.46204614639282227}, rmse loss - 14.517402386753322\n",
      "epoch - 19, gen loss - {'gen': 0.3767911195755005}, rmse loss - 14.602402804962383\n",
      "training end, best epoch - 0, best valid rmse loss - 14.386502966229767\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7447a020b4ce43f79868b2167560c128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.180 MB of 0.180 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▆▆█▁▃▄▂▃▄▆█▄▄▃▃▃▃▆▄▄▄▆▄▃▅▇▅▆▅▄▄▄▄▃▄▃▃▅▂▅</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>▇██▅▅▆▅▄▆▃▂▂▁▁▁▂▂▃▄▅▄▅▆▁▅▆▅▄▆▆▆▅▅▆▆▇▆▅▆▆</td></tr><tr><td>Gen_loss</td><td>█▅▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>rmse_loss</td><td>█▇▇▅▅▄▄▂▃▁▁▁▁▁▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00475</td></tr><tr><td>Dis_A_step_loss</td><td>-0.00771</td></tr><tr><td>Dis_B_loss</td><td>-0.00937</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01454</td></tr><tr><td>Gen_loss</td><td>0.39419</td></tr><tr><td>rmse_loss</td><td>14.6024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-snowball-13</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/e3vwcagu\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/e3vwcagu</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221221_000634-e3vwcagu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b10b8",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca18dcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20844404a8c742a4a38eaf336ad57d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668404949208102, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221221_011245-1tei2f92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/1tei2f92\" target=\"_blank\">dazzling-smoke-14</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.6679122447967529}, rmse loss - 14.617746733211503\n",
      "epoch - 1, gen loss - {'gen': 0.6631978750228882}, rmse loss - 14.821470060031793\n",
      "epoch - 2, gen loss - {'gen': 0.7074399590492249}, rmse loss - 14.645978135816286\n",
      "epoch - 3, gen loss - {'gen': 0.6233723163604736}, rmse loss - 14.78620805951502\n",
      "epoch - 4, gen loss - {'gen': 0.7990195751190186}, rmse loss - 14.682067476955288\n",
      "epoch - 5, gen loss - {'gen': 0.6818413734436035}, rmse loss - 14.766668646977836\n",
      "epoch - 6, gen loss - {'gen': 0.600968599319458}, rmse loss - 14.729183346582955\n",
      "epoch - 7, gen loss - {'gen': 0.46795231103897095}, rmse loss - 15.297438426211311\n",
      "epoch - 8, gen loss - {'gen': 0.4970361590385437}, rmse loss - 14.688044384396823\n",
      "epoch - 9, gen loss - {'gen': 0.48656919598579407}, rmse loss - 14.920291196816082\n",
      "epoch - 10, gen loss - {'gen': 0.5994480848312378}, rmse loss - 15.08745084565504\n",
      "epoch - 11, gen loss - {'gen': 0.4750051498413086}, rmse loss - 14.856936416062922\n",
      "epoch - 12, gen loss - {'gen': 0.5022153854370117}, rmse loss - 14.672656159559299\n",
      "epoch - 13, gen loss - {'gen': 0.281122088432312}, rmse loss - 14.734020431983074\n",
      "epoch - 14, gen loss - {'gen': 0.3946899473667145}, rmse loss - 14.83292117066049\n",
      "epoch - 15, gen loss - {'gen': 0.41876640915870667}, rmse loss - 14.963403890053725\n",
      "epoch - 16, gen loss - {'gen': 0.33214548230171204}, rmse loss - 14.97153351315713\n",
      "epoch - 17, gen loss - {'gen': 0.330946683883667}, rmse loss - 14.928058849489556\n",
      "epoch - 18, gen loss - {'gen': 0.2664809226989746}, rmse loss - 14.847267803670736\n",
      "epoch - 19, gen loss - {'gen': 0.21874308586120605}, rmse loss - 14.841845929402707\n",
      "training end, best epoch - 0, best valid rmse loss - 14.617746733211503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aa0e83f0c64aeab72ff13c4e40d151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.181 MB of 0.181 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▇▅▇▆▆▆▅█▄▇▅▆▄▃▅▇▄▃▄▂▄▅▁▂▃▄▄▂▄▂▃▂▂▅▅▃▃▁▃▁</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>██▇▇▇▆▆▆▆▅▅▃▂▃▃▅▂▅▁▂▃▃▃▃▅▇▇▃▄▅▄▅▇▂▅▃▃▇▃▅</td></tr><tr><td>Gen_loss</td><td>█▄▅▅▅▅▄▃▄▄▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>rmse_loss</td><td>▁▃▁▃▂▃▂█▂▄▆▃▂▂▃▅▅▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.0094</td></tr><tr><td>Dis_A_step_loss</td><td>-0.014</td></tr><tr><td>Dis_B_loss</td><td>-0.01621</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01415</td></tr><tr><td>Gen_loss</td><td>0.21453</td></tr><tr><td>rmse_loss</td><td>14.84185</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dazzling-smoke-14</strong>: <a href=\"https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/1tei2f92\" target=\"_blank\">https://wandb.ai/kimjiil2013/Samsung%20sem%20CycleGan/runs/1tei2f92</a><br/>Synced 6 W&B file(s), 40 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221221_011245-1tei2f92/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Samsung sem CycleGan\", entity=\"kimjiil2013\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'simtotrain')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65dfdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
