{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c680477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "import cv2, PIL\n",
    "import os, glob\n",
    "import csv, platform\n",
    "\n",
    "current_os = platform.system()\n",
    "if current_os == \"Linux\":\n",
    "    _path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "    cfg = {\n",
    "        'device': \"cuda:5\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 64,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 4,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "elif current_os == \"Windows\":\n",
    "    _path = 'D:/git_repos/samsung_sem'\n",
    "    cfg = {\n",
    "        'device': \"cuda:0\",\n",
    "        \"db_path\": _path,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 4,\n",
    "        'lr': 0.0002,\n",
    "        'num_workers': 0,\n",
    "        'n_fold': 5\n",
    "    }\n",
    "\n",
    "import wandb\n",
    "\n",
    "project_name = \"221227_Samsung_sem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c4e94",
   "metadata": {},
   "source": [
    "# Cycle Gan Model,  Generator:Resnet, Discriminator:PatchGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2307193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(resnet_block, self).__init__()\n",
    "\n",
    "        _resnet_block = [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(dim)\n",
    "        ]\n",
    "\n",
    "        self.layer = nn.Sequential(*_resnet_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x) + x\n",
    "        return out\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.donw_sampling_layer1 = nn.Sequential(\n",
    "            # down sampling\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.donw_sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.res_block = nn.Sequential(*[resnet_block(64) for i in range(3)])\n",
    "\n",
    "        self.up_samplig_layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.up_samplig_layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(16, input_ch, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.model(x)\n",
    "        out1 = self.init_layer(x)\n",
    "        out2 = self.donw_sampling_layer1(out1)\n",
    "        out3 = self.donw_sampling_layer2(out2)\n",
    "        out4 = self.res_block(out3)\n",
    "        out5 = self.up_samplig_layer1(out4)\n",
    "        out6 = self.up_samplig_layer2(out5)\n",
    "        out7 = self.output_layer(out6)\n",
    "        return out7\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "class PatchGanDiscriminator(nn.Module):\n",
    "    def __init__(self, input_ch):\n",
    "        super(PatchGanDiscriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_ch, 16, kernel_size=7, stride=1, padding=1, padding_mode='replicate'),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1, bias=True),  # 1\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=True),  # 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def set_requires_grad(self, mode):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'wgan_gp':\n",
    "            self.loss = None\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            if target_is_real:\n",
    "                target_tensor = self.real_label  # .to(self.device)\n",
    "            else:\n",
    "                target_tensor = self.fake_label  # .to(self.device)\n",
    "\n",
    "            target_tensor = target_tensor.expand_as(prediction)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "\n",
    "def _gradient_penalty(netD, real_data, fake_data, type=\"mixed\", constant=1.0, lambda_gp=10.0):\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':  # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1, device=real_data.device)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(\n",
    "                *real_data.shape)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(real_data.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "class cycleGAN_model(nn.Module):\n",
    "    def __init__(self, input_ch=3,\n",
    "                 optim_lr=0.0002,\n",
    "                 gan_mode='lsgan',\n",
    "                 guided=False):\n",
    "        import itertools\n",
    "\n",
    "        super(cycleGAN_model, self).__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.guided = guided\n",
    "\n",
    "        self.Gen = nn.ModuleDict({\n",
    "            'A': ResnetGenerator(input_ch),\n",
    "            'B': ResnetGenerator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Gen['A'], log='all')\n",
    "        # wandb.watch(self.Gen['B'], log='all')\n",
    "\n",
    "        self.Dis = nn.ModuleDict({\n",
    "            'A': PatchGanDiscriminator(input_ch),\n",
    "            'B': PatchGanDiscriminator(input_ch)\n",
    "        })\n",
    "\n",
    "        # wandb.watch(self.Dis['A'], log='all')\n",
    "        # wandb.watch(self.Dis['B'], log='all')\n",
    "\n",
    "        self.optimizer = {\n",
    "            'G': torch.optim.Adam(itertools.chain(self.Gen['A'].parameters(), self.Gen['B'].parameters()), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_A': torch.optim.Adam(self.Dis['A'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999)),\n",
    "            'D_B': torch.optim.Adam(self.Dis['B'].parameters(), lr=optim_lr,\n",
    "                                  betas=(0.5, 0.999))\n",
    "        }\n",
    "\n",
    "        self.schedular = {\n",
    "            'G': torch.optim.lr_scheduler.LambdaLR(self.optimizer['G'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_A': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_A'], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "            'D_B': torch.optim.lr_scheduler.LambdaLR(self.optimizer['D_B'], lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "        }\n",
    "\n",
    "        self.criterion = nn.ModuleDict({\n",
    "            'cycle': nn.L1Loss(),\n",
    "            'idt': nn.L1Loss(),\n",
    "            'gan': GANLoss(self.gan_mode),\n",
    "            'mse': nn.MSELoss(),\n",
    "            'guided': nn.L1Loss()\n",
    "        })\n",
    "\n",
    "        self.lambda_idt = 0.5\n",
    "        self.lambda_A = 10.0\n",
    "        self.lambda_B = 10.0\n",
    "\n",
    "    def forward(self, data_A, data_B, mode: str):\n",
    "        if mode == 'gen':\n",
    "            A_out = self.Gen['A'](data_A)\n",
    "            B_out = self.Gen['B'](data_B)\n",
    "        elif mode == 'dis':\n",
    "            A_out = self.Dis['A'](data_A)\n",
    "            B_out = self.Dis['B'](data_B)\n",
    "        else:\n",
    "            raise None\n",
    "        return A_out, B_out\n",
    "\n",
    "    def model_train_discriminator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', True)\n",
    "\n",
    "        self.optimizer['D_B'].zero_grad()\n",
    "\n",
    "        pred_real_B, pred_real_A = self(real_B, real_A, 'dis')  # netA netB\n",
    "        pred_fake_B, pred_fake_A = self(fake_B.detach(), fake_A.detach(), 'dis')\n",
    "\n",
    "        # Discriminator B update\n",
    "        loss_D_B_Real = self.criterion['gan'](pred_real_A, True)\n",
    "        loss_D_B_fake = self.criterion['gan'](pred_fake_A, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_B = (loss_D_B_fake + loss_D_B_Real) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_B = _gradient_penalty(self.Dis['B'], real_A, fake_A.detach())\n",
    "            loss_D_B = loss_D_B_fake + loss_D_B_Real + gradient_penalty_B[0]\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        self.optimizer['D_B'].step()\n",
    "\n",
    "        # Discriminator A update\n",
    "        self.optimizer['D_A'].zero_grad()\n",
    "\n",
    "        loss_D_A_Real = self.criterion['gan'](pred_real_B, True)\n",
    "        loss_D_A_fake = self.criterion['gan'](pred_fake_B, False)\n",
    "\n",
    "        if self.gan_mode == 'lsgan':\n",
    "            loss_D_A = (loss_D_A_Real + loss_D_A_fake) * 0.5\n",
    "        elif self.gan_mode == 'wgan_gp':\n",
    "            gradient_penalty_A = _gradient_penalty(self.Dis['A'], real_B, fake_B.detach())\n",
    "            loss_D_A = loss_D_A_Real + loss_D_A_fake + gradient_penalty_A[0]\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        self.optimizer['D_A'].step()\n",
    "\n",
    "        loss_dic = {'dis_a': loss_D_A.item(),\n",
    "                    'dis_b': loss_D_B.item()}\n",
    "\n",
    "        return loss_dic\n",
    "\n",
    "    def model_train_generator(self, real_A, real_B):\n",
    "        self.train()\n",
    "\n",
    "        fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "        rec_B, rec_A = self(fake_A, fake_B, 'gen')\n",
    "\n",
    "        self.set_requires_grad('dis', False)\n",
    "        self.optimizer['G'].zero_grad()\n",
    "\n",
    "        idt_A, idt_B = self(real_B, real_A, 'gen')\n",
    "\n",
    "        loss_idt_A = self.criterion['idt'](idt_A, real_B) * self.lambda_B * self.lambda_idt\n",
    "        loss_idt_B = self.criterion['idt'](idt_B, real_A) * self.lambda_A * self.lambda_idt\n",
    "\n",
    "        dis_A_fake_B, dis_B_fake_A = self(fake_B, fake_A, 'dis')  # dis_A(fake_B) / dis_B(fake_A)\n",
    "\n",
    "        loss_G_A = self.criterion['gan'](dis_A_fake_B, True)\n",
    "        loss_G_B = self.criterion['gan'](dis_B_fake_A, True)\n",
    "\n",
    "        loss_cycle_A = self.criterion['cycle'](rec_A, real_A) * self.lambda_A\n",
    "        loss_cycle_B = self.criterion['cycle'](rec_B, real_B) * self.lambda_B\n",
    "\n",
    "        # Guied Loss (paired)\n",
    "        if self.guided:\n",
    "            loss_guided_A = self.criterion['guided'](fake_B, real_B)\n",
    "            loss_guided_B = self.criterion['guided'](fake_A, real_A)\n",
    "        else:\n",
    "            loss_guided_A = 0\n",
    "            loss_guided_B = 0\n",
    "        ##########\n",
    "\n",
    "        loss_Gen = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B + loss_guided_A + loss_guided_B\n",
    "        loss_Gen.backward()\n",
    "\n",
    "        self.optimizer['G'].step()\n",
    "\n",
    "        loss_dic = {'gen': loss_Gen.item()}\n",
    "\n",
    "        inference_image = {\n",
    "            'real_a': real_A,\n",
    "            'real_b': real_B,\n",
    "            'atob_fake': fake_B,\n",
    "            'btoa_fake': fake_A,\n",
    "            'rec_a': rec_A,\n",
    "            'rec_b': rec_B\n",
    "        }\n",
    "\n",
    "        return loss_dic, {key: self.tensortonp(inference_image[key]) for key in inference_image}\n",
    "\n",
    "    def model_valid(self, real_A, real_B):\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_B, fake_A = self(real_A, real_B, 'gen')\n",
    "\n",
    "            true = (real_B * 255).type(torch.uint8).float()\n",
    "            fake_true = (fake_B * 255).type(torch.uint8).float()\n",
    "            rmse_loss = torch.sqrt(self.criterion['mse'](fake_true, true))\n",
    "\n",
    "        img_dict = {\n",
    "            'real_A': real_A,\n",
    "            'fake_B': fake_B,\n",
    "\n",
    "            'real_B': real_B,\n",
    "            'fake_A': fake_A,\n",
    "        }\n",
    "\n",
    "        return rmse_loss.item(), {key: self.tensortonp(img_dict[key]) for key in img_dict}\n",
    "\n",
    "    def tensortonp(self, tensor):\n",
    "        return (tensor.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    def set_requires_grad(self, net_type='dis', mode=True):\n",
    "        if net_type == 'gen':\n",
    "            net_dic = self.Gen\n",
    "        elif net_type == 'dis':\n",
    "            net_dic = self.Dis\n",
    "\n",
    "        for key in net_dic:\n",
    "            net_dic[key].set_requires_grad(mode)\n",
    "\n",
    "    def schedular_step(self):\n",
    "        self.schedular['G'].step()\n",
    "        self.schedular['D_A'].step()\n",
    "        self.schedular['D_B'].step()\n",
    "\n",
    "    def model_save(self, PATH):\n",
    "        temp_dict = {}\n",
    "        key_list = [key for key in self.__dict__.keys() if not '_' in key[0]]\n",
    "        key_list.extend([key for key in self.__dict__['_modules'].keys()])\n",
    "\n",
    "        for key in key_list:\n",
    "            if hasattr(self, key):\n",
    "                value = getattr(self, key)\n",
    "                if isinstance(value, dict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = {}\n",
    "                    for sub_key in value.keys():\n",
    "                        if not sub_key in temp_dict[key]:\n",
    "                            temp_dict[key][sub_key] = value[sub_key].state_dict()\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value.state_dict()\n",
    "                else:\n",
    "                    if not key in temp_dict:\n",
    "                        temp_dict[key] = value\n",
    "\n",
    "        torch.save(temp_dict, PATH)\n",
    "\n",
    "    def model_load(self, PATH, device):\n",
    "        state_dict = torch.load(PATH, map_location=device)\n",
    "\n",
    "        for cls_key in state_dict.keys():\n",
    "            if hasattr(self, cls_key):\n",
    "                value = getattr(self, cls_key)\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key in value.keys():\n",
    "                        value[sub_key].load_state_dict(state_dict[cls_key][sub_key])\n",
    "                elif isinstance(value, nn.ModuleDict):\n",
    "                    value.load_state_dict(state_dict[cls_key])\n",
    "                else:\n",
    "                    setattr(self, cls_key, state_dict[cls_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8f2ce",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f2c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list(abs_path):\n",
    "    # abs_path = '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset'\n",
    "\n",
    "    # Dataset path\n",
    "    sim_depth_path = os.path.join(abs_path, 'simulation_data/Depth')\n",
    "    sim_sem_path = os.path.join(abs_path, 'simulation_data/SEM')\n",
    "\n",
    "    train_path = os.path.join(abs_path, 'train')\n",
    "\n",
    "    # only Test\n",
    "    test_path = os.path.join(abs_path, 'test/SEM')\n",
    "\n",
    "    sim_depth_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_depth_path):\n",
    "        if not case in sim_depth_img_path_dic:\n",
    "            sim_depth_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_depth_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_depth_path, case, folder, '*.png'))\n",
    "            for img in img_list:\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "                sim_depth_img_path_dic[case].append(img)\n",
    "\n",
    "    sim_sem_img_path_dic = dict()\n",
    "    for case in os.listdir(sim_sem_path):\n",
    "        if not case in sim_sem_img_path_dic:\n",
    "            sim_sem_img_path_dic[case] = []\n",
    "        for folder in os.listdir(os.path.join(sim_sem_path, case)):\n",
    "            img_list = glob.glob(os.path.join(sim_sem_path, case, folder, '*.png'))\n",
    "            sim_sem_img_path_dic[case].extend(img_list)\n",
    "\n",
    "    train_avg_depth = dict()\n",
    "    with open(os.path.join(train_path, \"average_depth.csv\"), 'r') as csvfile:\n",
    "        temp = csv.reader(csvfile)\n",
    "        for idx, line in enumerate(temp):\n",
    "            if idx > 0:\n",
    "                depth_key, site_key = line[0].split('_site')\n",
    "                depth_key = depth_key.replace(\"d\", \"D\")\n",
    "                site_key = \"site\" + site_key\n",
    "                if not depth_key in train_avg_depth:\n",
    "                    train_avg_depth[depth_key] = dict()\n",
    "\n",
    "                train_avg_depth[depth_key][site_key] = float(line[1])\n",
    "\n",
    "    train_img_path_dic = dict()\n",
    "    for depth in os.listdir(os.path.join(train_path, \"SEM\")):\n",
    "        if not depth in train_img_path_dic:\n",
    "            train_img_path_dic[depth] = []\n",
    "        for site in os.listdir(os.path.join(train_path, \"SEM\", depth)):\n",
    "            img_list = glob.glob(os.path.join(train_path, \"SEM\", depth, site, \"*.png\"))\n",
    "            train_img_path_dic[depth].extend([[temp_img, train_avg_depth[depth][site]] for temp_img in img_list])\n",
    "\n",
    "    test_img_path_list = glob.glob(os.path.join(test_path, \"*.png\"))\n",
    "\n",
    "    result_dic = dict()\n",
    "    result_dic['sim'] = dict()\n",
    "    result_dic['sim']['sem'] = sim_sem_img_path_dic\n",
    "    result_dic['sim']['depth'] = sim_depth_img_path_dic\n",
    "    result_dic['train'] = train_img_path_dic\n",
    "    result_dic['test'] = np.array(test_img_path_list)\n",
    "    result_dic['train_avg_depth'] = train_avg_depth\n",
    "\n",
    "    return result_dic\n",
    "\n",
    "result_dic = get_img_list(cfg['db_path'])\n",
    "\n",
    "'''\n",
    "train sem ->  sim sem -> sim depth\n",
    "\n",
    "case 별로 dataset을 나눠야됨.\n",
    "'''\n",
    "\n",
    "class gan_dataset(Dataset):\n",
    "    def __init__(self, a_data_path, b_data_path, transform=None):\n",
    "        super(gan_dataset, self).__init__()\n",
    "        self.a_data_path = a_data_path\n",
    "        self.b_data_path = b_data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.a_size = len(a_data_path)\n",
    "        self.b_size = len(b_data_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.a_size > self.b_size:\n",
    "            a_idx = idx\n",
    "            b_idx = idx % self.b_size\n",
    "        else:\n",
    "            a_idx = idx % self.a_size\n",
    "            b_idx = idx\n",
    "        if isinstance(self.a_data_path[a_idx], str):\n",
    "            a_path = self.a_data_path[a_idx]\n",
    "        elif isinstance(self.a_data_path[a_idx], list):\n",
    "            a_path = self.a_data_path[a_idx][0]\n",
    "\n",
    "        if isinstance(self.b_data_path[b_idx], str):\n",
    "            b_path = self.b_data_path[b_idx]\n",
    "        elif isinstance(self.b_data_path[b_idx], list):\n",
    "            b_path = self.b_data_path[b_idx][0]\n",
    "\n",
    "        a_img = PIL.Image.open(a_path).convert(\"L\")\n",
    "        b_img = PIL.Image.open(b_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            a_img = self.transform(a_img)\n",
    "            b_img = self.transform(b_img)\n",
    "\n",
    "        a_img = (np.array(a_img) / 255.)\n",
    "        a_img = a_img.reshape(1, *a_img.shape).astype(np.float32)\n",
    "        b_img = (np.array(b_img) / 255.)\n",
    "        b_img = b_img.reshape(1, *b_img.shape).astype(np.float32)\n",
    "\n",
    "        return a_img, b_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.a_data_path), len(self.b_data_path))\n",
    "\n",
    "def create_dataloader(a_key, b_key, t_ratio, result_dic, case=1):\n",
    "    if 'sim' in a_key:\n",
    "        a_list = result_dic['sim'][a_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        a_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "    if 'sim' in b_key:\n",
    "        b_list = result_dic['sim'][b_key.split('_')[-1]][f\"Case_{case}\"]\n",
    "    else:\n",
    "        b_list = result_dic['train'][f\"Depth_{100 + 10 * case}\"]\n",
    "\n",
    "    horizon_transform = transforms.RandomHorizontalFlip(1.0)\n",
    "    rotate_transform = transforms.RandomRotation((180, 180))\n",
    "    vertical_transform = transforms.RandomVerticalFlip(1.0)\n",
    "\n",
    "    a_train_data_size = int(len(a_list) * t_ratio)\n",
    "    b_train_data_size = int(len(b_list) * t_ratio)\n",
    "\n",
    "    train_dataset = gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], None) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[:a_train_data_size], b_list[:b_train_data_size], vertical_transform)\n",
    "\n",
    "    valid_dataset = gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], None) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], horizon_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], rotate_transform) + \\\n",
    "                    gan_dataset(a_list[a_train_data_size:], b_list[b_train_data_size:], vertical_transform)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=True), \\\n",
    "           DataLoader(valid_dataset, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb4a18",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac7389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def valid(model, valid_dataloader, device):\n",
    "    rmse_list = []\n",
    "    img_mean_list = []\n",
    "    for step_i, data_tuple in enumerate(valid_dataloader):\n",
    "        real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "        real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "        rmse_loss, img_dict = model.model_valid(real_a, real_b)\n",
    "        rmse_list.append(rmse_loss)\n",
    "        if step_i == 0:\n",
    "            img_list = [img_dict[key][0][0] for key in img_dict]\n",
    "            img_list = [wandb.Image(PIL.Image.fromarray(np.concatenate((img_list[i], img_list[i+1]), axis=-1)).convert('L'), caption=key)\n",
    "                        for i, key in enumerate(img_dict.keys()) if i % 2 == 0]\n",
    "            wandb.log({\n",
    "                \"example image\": img_list\n",
    "            })\n",
    "        \n",
    "    \n",
    "        img_mean_list.extend(list(np.mean(img_dict['fake_B'], axis=(1,2,3))))\n",
    "        \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.hist(img_mean_list, bins=100, density=True, alpha=0.5, color=plt.cm.tab20c(1))\n",
    "    plt.xlim(80, 200)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({\n",
    "        'plot': wandb.Image(fig)\n",
    "    })\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "def training(case, epochs, device, type, guided ,checkpoint_path=None):\n",
    "    best_rmse_loss = 9999\n",
    "    critic_iter = 5\n",
    "    best_epoch = 0\n",
    "\n",
    "    if type == 'semtodepth':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'sim_depth'\n",
    "    elif type == 'simtotrain':\n",
    "        a_key = 'sim_sem'\n",
    "        b_key = 'train'\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloader(a_key=a_key,\n",
    "                                                           b_key=b_key,\n",
    "                                                           t_ratio=0.8,\n",
    "                                                           result_dic=result_dic,\n",
    "                                                           case=case)\n",
    "\n",
    "    model = cycleGAN_model(1, optim_lr=0.0002, gan_mode='wgan_gp', guided=guided)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        model.model_load(checkpoint_path, device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_list = [[], [], []]\n",
    "        for step_i, data_tuple in enumerate(train_dataloader):\n",
    "            real_a = data_tuple[0].to(device, non_blocking=True)\n",
    "            real_b = data_tuple[1].to(device, non_blocking=True)\n",
    "\n",
    "            dis_loss = model.model_train_discriminator(real_a, real_b)\n",
    "            loss_list[1].append(dis_loss['dis_a'])\n",
    "            loss_list[2].append(dis_loss['dis_b'])\n",
    "            if step_i % critic_iter == 0:\n",
    "                gen_loss, img_dic = model.model_train_generator(real_a, real_b)\n",
    "                loss_list[0].append(gen_loss['gen'])\n",
    "\n",
    "                wandb.log({\n",
    "                    'Gen_step_loss': gen_loss,\n",
    "                    'Dis_A_step_loss': dis_loss['dis_a'],\n",
    "                    'Dis_B_step_loss': dis_loss['dis_b']\n",
    "                })\n",
    "\n",
    "            \n",
    "        rmse_loss = valid(model, valid_dataloader, device)\n",
    "        print(f'epoch - {epoch}, gen loss - {gen_loss}, rmse loss - {rmse_loss}')\n",
    "        wandb.log({\n",
    "            'Gen_loss': np.mean(loss_list[0]),\n",
    "            'Dis_A_loss': np.mean(loss_list[1]),\n",
    "            'Dis_B_loss': np.mean(loss_list[2]),\n",
    "            'learning_rate': model.schedular['G'].get_last_lr(),\n",
    "            'rmse_loss': rmse_loss\n",
    "        })\n",
    "\n",
    "        if best_rmse_loss > rmse_loss:\n",
    "            best_rmse_loss = rmse_loss\n",
    "            model.model_save(f'./case{case}_t({type})_best_model.pth')\n",
    "\n",
    "        model.schedular_step()\n",
    "    print(f'training end, best epoch - {best_epoch}, best valid rmse loss - {best_rmse_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282a077",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #1 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a235cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_160736-g4nqgj0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/g4nqgj0j\" target=\"_blank\">semtodepth_c1</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 1.0379643440246582}, rmse loss - 9.922977866281883\n",
      "epoch - 1, gen loss - {'gen': 0.9865840077400208}, rmse loss - 9.445224395977174\n",
      "epoch - 2, gen loss - {'gen': 1.0307201147079468}, rmse loss - 9.19954258049546\n",
      "epoch - 3, gen loss - {'gen': 1.164305567741394}, rmse loss - 8.986725249413634\n",
      "epoch - 4, gen loss - {'gen': 1.1354504823684692}, rmse loss - 8.811310095100826\n",
      "epoch - 5, gen loss - {'gen': 1.0747076272964478}, rmse loss - 8.55147104157733\n",
      "epoch - 6, gen loss - {'gen': 1.1227301359176636}, rmse loss - 8.443565941384797\n",
      "epoch - 7, gen loss - {'gen': 1.0541313886642456}, rmse loss - 8.260078565660878\n",
      "epoch - 8, gen loss - {'gen': 0.9875705242156982}, rmse loss - 8.138526282187318\n",
      "epoch - 9, gen loss - {'gen': 1.1196106672286987}, rmse loss - 8.056576046996451\n",
      "epoch - 10, gen loss - {'gen': 1.0708122253417969}, rmse loss - 8.057947494886898\n",
      "epoch - 11, gen loss - {'gen': 0.8867712616920471}, rmse loss - 7.771504817413668\n",
      "epoch - 12, gen loss - {'gen': 1.03997004032135}, rmse loss - 7.77367518924699\n",
      "epoch - 13, gen loss - {'gen': 0.9506022334098816}, rmse loss - 7.716687976654165\n",
      "epoch - 14, gen loss - {'gen': 0.9249204397201538}, rmse loss - 7.68525644070108\n",
      "epoch - 15, gen loss - {'gen': 0.9337248802185059}, rmse loss - 7.751902952405359\n",
      "epoch - 16, gen loss - {'gen': 0.7043840289115906}, rmse loss - 7.632527028502573\n",
      "epoch - 17, gen loss - {'gen': 0.5910015106201172}, rmse loss - 7.64764391452184\n",
      "epoch - 18, gen loss - {'gen': 0.7423455119132996}, rmse loss - 7.5886257213859984\n",
      "epoch - 19, gen loss - {'gen': 0.5773278474807739}, rmse loss - 7.626672065565947\n",
      "epoch - 20, gen loss - {'gen': 0.6707175970077515}, rmse loss - 7.657674142795296\n",
      "epoch - 21, gen loss - {'gen': 0.6002808809280396}, rmse loss - 7.648819325154998\n",
      "epoch - 22, gen loss - {'gen': 0.667728066444397}, rmse loss - 7.610036467274177\n",
      "epoch - 23, gen loss - {'gen': 0.6319057941436768}, rmse loss - 7.648373954850369\n",
      "epoch - 24, gen loss - {'gen': 0.5654867887496948}, rmse loss - 7.652222122213497\n",
      "epoch - 25, gen loss - {'gen': 0.42056751251220703}, rmse loss - 7.634690267133537\n",
      "epoch - 26, gen loss - {'gen': 0.5128639936447144}, rmse loss - 7.604276899921938\n",
      "epoch - 27, gen loss - {'gen': 0.6087671518325806}, rmse loss - 7.598896408432964\n",
      "epoch - 28, gen loss - {'gen': 0.6479017734527588}, rmse loss - 7.580053452636043\n",
      "epoch - 29, gen loss - {'gen': 0.6045648455619812}, rmse loss - 7.597754320095386\n",
      "epoch - 30, gen loss - {'gen': 0.6208699345588684}, rmse loss - 7.524901115586397\n",
      "epoch - 31, gen loss - {'gen': 0.6303048133850098}, rmse loss - 7.58121092732982\n",
      "epoch - 32, gen loss - {'gen': 0.6818188428878784}, rmse loss - 7.641532070962265\n",
      "epoch - 33, gen loss - {'gen': 0.6853894591331482}, rmse loss - 7.642385229413359\n",
      "epoch - 34, gen loss - {'gen': 0.6114522218704224}, rmse loss - 7.580384909006942\n",
      "epoch - 35, gen loss - {'gen': 0.5952332019805908}, rmse loss - 7.624506439230099\n",
      "epoch - 36, gen loss - {'gen': 0.6366184949874878}, rmse loss - 7.648882839512561\n",
      "epoch - 37, gen loss - {'gen': 0.6603046655654907}, rmse loss - 7.59818959236145\n",
      "epoch - 38, gen loss - {'gen': 0.6174234747886658}, rmse loss - 7.6111394628827425\n",
      "epoch - 39, gen loss - {'gen': 0.6466643810272217}, rmse loss - 7.643973957568517\n",
      "epoch - 40, gen loss - {'gen': 0.615197479724884}, rmse loss - 7.5489143709415\n",
      "epoch - 41, gen loss - {'gen': 0.5900821685791016}, rmse loss - 7.593389067702628\n",
      "epoch - 42, gen loss - {'gen': 0.5195765495300293}, rmse loss - 7.593302609735749\n",
      "epoch - 43, gen loss - {'gen': 0.5698645710945129}, rmse loss - 7.627013188886466\n",
      "epoch - 44, gen loss - {'gen': 0.5392398238182068}, rmse loss - 7.610314261869311\n",
      "epoch - 45, gen loss - {'gen': 0.5333955883979797}, rmse loss - 7.594821750457876\n",
      "epoch - 46, gen loss - {'gen': 0.47745242714881897}, rmse loss - 7.634904481388106\n",
      "epoch - 47, gen loss - {'gen': 0.5218274593353271}, rmse loss - 7.624278305201513\n",
      "epoch - 48, gen loss - {'gen': 0.4739190936088562}, rmse loss - 7.633641936242361\n",
      "epoch - 49, gen loss - {'gen': 0.48657840490341187}, rmse loss - 7.590974377530087\n",
      "training end, best epoch - 0, best valid rmse loss - 7.524901115586397\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▅▂▄▄▆▄█▆▆▅▅▆▆▂▅▃▃▆▅▁▄▆▄▂▅▃▅▂▃▄▂▅▆▄▁▃▂▄▁▅</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>▇▆▅▄▅▁▆█▅▆▆▆▆▅▆▆▆▇▅▆▇▅▆▆▇▆▆▆▆▅▆▆▆▇▆▅▆▅▄▅</td></tr><tr><td>Gen_loss</td><td>█▅▅▅▆▅▅▅▅▄▄▄▄▄▃▃▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00441</td></tr><tr><td>Dis_A_step_loss</td><td>0.01081</td></tr><tr><td>Dis_B_loss</td><td>-0.00395</td></tr><tr><td>Dis_B_step_loss</td><td>0.00217</td></tr><tr><td>Gen_loss</td><td>0.48799</td></tr><tr><td>rmse_loss</td><td>7.59097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">semtodepth_c1</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/g4nqgj0j\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/g4nqgj0j</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_160736-g4nqgj0j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"semtodepth_c1\")\n",
    "training(1, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263190d9",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #2 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e009ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_160858-3b747v5j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3b747v5j\" target=\"_blank\">semtodepth_c2</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 1.129433512687683}, rmse loss - 11.936002984697968\n",
      "epoch - 1, gen loss - {'gen': 1.0094826221466064}, rmse loss - 10.69811074408218\n",
      "epoch - 2, gen loss - {'gen': 0.9620792865753174}, rmse loss - 10.258456048930263\n",
      "epoch - 3, gen loss - {'gen': 0.7936031222343445}, rmse loss - 10.296285560650139\n",
      "epoch - 4, gen loss - {'gen': 0.7312467694282532}, rmse loss - 9.969710321883873\n",
      "epoch - 5, gen loss - {'gen': 0.7555220127105713}, rmse loss - 9.559059364769292\n",
      "epoch - 6, gen loss - {'gen': 0.6315388083457947}, rmse loss - 9.517359174045689\n",
      "epoch - 7, gen loss - {'gen': 0.6281772255897522}, rmse loss - 9.214861857495185\n",
      "epoch - 8, gen loss - {'gen': 0.7077594995498657}, rmse loss - 9.102339892369795\n",
      "epoch - 9, gen loss - {'gen': 0.5027087926864624}, rmse loss - 9.086325851313742\n",
      "epoch - 10, gen loss - {'gen': 0.6986837387084961}, rmse loss - 8.92689421520022\n",
      "epoch - 11, gen loss - {'gen': 0.6494389176368713}, rmse loss - 8.915030256848494\n",
      "epoch - 12, gen loss - {'gen': 0.6711381077766418}, rmse loss - 8.837233069198158\n",
      "epoch - 13, gen loss - {'gen': 0.5973721146583557}, rmse loss - 8.865383073412625\n",
      "epoch - 14, gen loss - {'gen': 0.5367281436920166}, rmse loss - 8.96053021948276\n",
      "epoch - 15, gen loss - {'gen': 0.5741926431655884}, rmse loss - 8.793965864885337\n",
      "epoch - 16, gen loss - {'gen': 0.6022937297821045}, rmse loss - 8.805000875268915\n",
      "epoch - 17, gen loss - {'gen': 0.6022369861602783}, rmse loss - 8.679316587553693\n",
      "epoch - 18, gen loss - {'gen': 0.6639096736907959}, rmse loss - 8.782300397478787\n",
      "epoch - 19, gen loss - {'gen': 0.5582396388053894}, rmse loss - 8.669390616821627\n",
      "epoch - 20, gen loss - {'gen': 0.5113899111747742}, rmse loss - 8.724295121717278\n",
      "epoch - 21, gen loss - {'gen': 0.42749860882759094}, rmse loss - 8.736845971033581\n",
      "epoch - 22, gen loss - {'gen': 0.48011839389801025}, rmse loss - 8.746918878872016\n",
      "epoch - 23, gen loss - {'gen': 0.33681878447532654}, rmse loss - 8.668434914627639\n",
      "epoch - 24, gen loss - {'gen': 0.4495323598384857}, rmse loss - 8.786341648700052\n",
      "epoch - 25, gen loss - {'gen': 0.4174734950065613}, rmse loss - 8.696801698515776\n",
      "epoch - 26, gen loss - {'gen': 0.5130916833877563}, rmse loss - 8.700729245189372\n",
      "epoch - 27, gen loss - {'gen': 0.5319221019744873}, rmse loss - 8.743487073046694\n",
      "epoch - 28, gen loss - {'gen': 0.3550565540790558}, rmse loss - 8.639845219925322\n",
      "epoch - 29, gen loss - {'gen': 0.488160640001297}, rmse loss - 8.765387213098167\n",
      "epoch - 30, gen loss - {'gen': 0.4578193724155426}, rmse loss - 8.67011919760616\n",
      "epoch - 31, gen loss - {'gen': 0.4760512709617615}, rmse loss - 8.657709113786142\n",
      "epoch - 32, gen loss - {'gen': 0.4873398542404175}, rmse loss - 8.628935028266202\n",
      "epoch - 33, gen loss - {'gen': 0.44023868441581726}, rmse loss - 8.750725357294963\n",
      "epoch - 34, gen loss - {'gen': 0.4073312282562256}, rmse loss - 8.692946230793352\n",
      "epoch - 35, gen loss - {'gen': 0.4743044674396515}, rmse loss - 8.671882594203597\n",
      "epoch - 36, gen loss - {'gen': 0.4212009012699127}, rmse loss - 8.709789096649283\n",
      "epoch - 37, gen loss - {'gen': 0.3909183144569397}, rmse loss - 8.62994688199455\n",
      "epoch - 38, gen loss - {'gen': 0.4835399091243744}, rmse loss - 8.625767942724194\n",
      "epoch - 39, gen loss - {'gen': 0.40071719884872437}, rmse loss - 8.696839005304879\n",
      "epoch - 40, gen loss - {'gen': 0.4126721918582916}, rmse loss - 8.703271940625461\n",
      "epoch - 41, gen loss - {'gen': 0.39200669527053833}, rmse loss - 8.750096976537106\n",
      "epoch - 42, gen loss - {'gen': 0.4327112138271332}, rmse loss - 8.693011050734572\n",
      "epoch - 43, gen loss - {'gen': 0.4338480532169342}, rmse loss - 8.728988175902419\n",
      "epoch - 44, gen loss - {'gen': 0.44543009996414185}, rmse loss - 8.671346257093647\n",
      "epoch - 45, gen loss - {'gen': 0.3983268141746521}, rmse loss - 8.70493659234135\n",
      "epoch - 46, gen loss - {'gen': 0.46436548233032227}, rmse loss - 8.705195291455821\n",
      "epoch - 47, gen loss - {'gen': 0.40071868896484375}, rmse loss - 8.716417465702634\n",
      "epoch - 48, gen loss - {'gen': 0.3669811487197876}, rmse loss - 8.701059666067032\n",
      "epoch - 49, gen loss - {'gen': 0.3496967852115631}, rmse loss - 8.738379064961114\n",
      "training end, best epoch - 0, best valid rmse loss - 8.625767942724194\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▆▄▇▃█▄▅▁▃▃▅▅▃▂▄▁▆▄▂▃▅▃▃▅▄▂▅▃▃▂▃▆▂▅▃▆▆▅▅▁</td></tr><tr><td>Dis_B_loss</td><td>█▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Dis_B_step_loss</td><td>▇▆▅▁▁▄▆▆▆▇▆▆▅▆▇▆▆▇▇▆▇▇▆▇▇▆▅▆▆▆▆▅▆▅▇▇▆▇█▆</td></tr><tr><td>Gen_loss</td><td>█▅▅▄▃▃▃▃▂▃▃▃▂▂▂▃▂▁▁▁▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▅▄▅▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.0046</td></tr><tr><td>Dis_A_step_loss</td><td>-0.00501</td></tr><tr><td>Dis_B_loss</td><td>-0.00548</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01851</td></tr><tr><td>Gen_loss</td><td>0.34842</td></tr><tr><td>rmse_loss</td><td>8.73838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">semtodepth_c2</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3b747v5j\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3b747v5j</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_160858-3b747v5j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"semtodepth_c2\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8283be3",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #3 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703e533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_160909-24xy8g59</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24xy8g59\" target=\"_blank\">semtodepth_c3</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.8269416093826294}, rmse loss - 11.945897679487278\n",
      "epoch - 1, gen loss - {'gen': 0.7820778489112854}, rmse loss - 11.519457561943364\n",
      "epoch - 2, gen loss - {'gen': 0.9086331129074097}, rmse loss - 10.81839163804846\n",
      "epoch - 3, gen loss - {'gen': 0.8253433108329773}, rmse loss - 10.592001337846707\n",
      "epoch - 4, gen loss - {'gen': 0.9413123726844788}, rmse loss - 10.362369030604063\n",
      "epoch - 5, gen loss - {'gen': 1.0336304903030396}, rmse loss - 10.414705600245853\n",
      "epoch - 6, gen loss - {'gen': 0.9275076985359192}, rmse loss - 10.137042550583168\n",
      "epoch - 7, gen loss - {'gen': 0.8957818746566772}, rmse loss - 9.717079157319017\n",
      "epoch - 8, gen loss - {'gen': 0.9087523818016052}, rmse loss - 9.55042094762035\n",
      "epoch - 9, gen loss - {'gen': 0.96787029504776}, rmse loss - 9.773470322584314\n",
      "epoch - 10, gen loss - {'gen': 0.8811497688293457}, rmse loss - 9.417069206378557\n",
      "epoch - 11, gen loss - {'gen': 1.0183578729629517}, rmse loss - 9.498465163241454\n",
      "epoch - 12, gen loss - {'gen': 0.8643640875816345}, rmse loss - 9.375178537685493\n",
      "epoch - 13, gen loss - {'gen': 0.8988853096961975}, rmse loss - 9.454720509448174\n",
      "epoch - 14, gen loss - {'gen': 0.9571688175201416}, rmse loss - 9.427603403140697\n",
      "epoch - 15, gen loss - {'gen': 0.9872243404388428}, rmse loss - 9.371631683898588\n",
      "epoch - 16, gen loss - {'gen': 0.8199174404144287}, rmse loss - 9.462356546268252\n",
      "epoch - 17, gen loss - {'gen': 0.9402073621749878}, rmse loss - 9.64781700465072\n",
      "epoch - 18, gen loss - {'gen': 0.7968426942825317}, rmse loss - 9.515672340604212\n",
      "epoch - 19, gen loss - {'gen': 0.9347601532936096}, rmse loss - 9.520863935956216\n",
      "epoch - 20, gen loss - {'gen': 0.8461904525756836}, rmse loss - 9.53716454382752\n",
      "epoch - 21, gen loss - {'gen': 0.7868231534957886}, rmse loss - 9.468887015902249\n",
      "epoch - 22, gen loss - {'gen': 0.6794613003730774}, rmse loss - 9.408650204704257\n",
      "epoch - 23, gen loss - {'gen': 0.8408375382423401}, rmse loss - 9.543646902175848\n",
      "epoch - 24, gen loss - {'gen': 0.680176854133606}, rmse loss - 9.554247981067952\n",
      "epoch - 25, gen loss - {'gen': 0.7172018885612488}, rmse loss - 9.536123312707316\n",
      "epoch - 26, gen loss - {'gen': 0.741927444934845}, rmse loss - 9.664183801390587\n",
      "epoch - 27, gen loss - {'gen': 0.765798807144165}, rmse loss - 9.55026733567354\n",
      "epoch - 28, gen loss - {'gen': 0.7433651089668274}, rmse loss - 9.646964279048117\n",
      "epoch - 29, gen loss - {'gen': 0.7431748509407043}, rmse loss - 9.533337276360205\n",
      "epoch - 30, gen loss - {'gen': 0.4879743158817291}, rmse loss - 9.588735450237879\n",
      "epoch - 31, gen loss - {'gen': 0.6003080010414124}, rmse loss - 9.528249995734859\n",
      "epoch - 32, gen loss - {'gen': 0.5867495536804199}, rmse loss - 9.582535017020588\n",
      "epoch - 33, gen loss - {'gen': 0.5764532685279846}, rmse loss - 9.546510904037644\n",
      "epoch - 34, gen loss - {'gen': 0.6004782319068909}, rmse loss - 9.608953329909772\n",
      "epoch - 35, gen loss - {'gen': 0.6149875521659851}, rmse loss - 9.639689691832144\n",
      "epoch - 36, gen loss - {'gen': 0.559489905834198}, rmse loss - 9.647721257156991\n",
      "epoch - 37, gen loss - {'gen': 0.610450804233551}, rmse loss - 9.694057119728454\n",
      "epoch - 38, gen loss - {'gen': 0.624756932258606}, rmse loss - 9.640046871016386\n",
      "epoch - 39, gen loss - {'gen': 0.6634787917137146}, rmse loss - 9.588635690977652\n",
      "epoch - 40, gen loss - {'gen': 0.6294602751731873}, rmse loss - 9.69164642812581\n",
      "epoch - 41, gen loss - {'gen': 0.6764418482780457}, rmse loss - 9.616444471577436\n",
      "epoch - 42, gen loss - {'gen': 0.6117352843284607}, rmse loss - 9.663245012839342\n",
      "epoch - 43, gen loss - {'gen': 0.5870715975761414}, rmse loss - 9.69312582684619\n",
      "epoch - 44, gen loss - {'gen': 0.6301420331001282}, rmse loss - 9.730080166426092\n",
      "epoch - 45, gen loss - {'gen': 0.6110786199569702}, rmse loss - 9.65996352508939\n",
      "epoch - 46, gen loss - {'gen': 0.5827223062515259}, rmse loss - 9.670368757635025\n",
      "epoch - 47, gen loss - {'gen': 0.599029541015625}, rmse loss - 9.675915292268309\n",
      "epoch - 48, gen loss - {'gen': 0.6527999639511108}, rmse loss - 9.633869343578155\n",
      "epoch - 49, gen loss - {'gen': 0.626655101776123}, rmse loss - 9.65646249165834\n",
      "training end, best epoch - 0, best valid rmse loss - 9.371631683898588\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▅▆▆▅█▇█▇▆▅▅▄▆▇▅▇▂█▄▆▂▅▄▆▇▆▅▄▃▄▅▁█▄▅▆▆▅▅</td></tr><tr><td>Dis_B_loss</td><td>█▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Dis_B_step_loss</td><td>▆▇▇▄▁▁▇▆▇▇▇▇▇█▇█▇▇▇█▆▆▇▆▆▇██▇▇▇▇▇▆▆▇▇▆█▇</td></tr><tr><td>Gen_loss</td><td>█▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▂▂▂▂▃▃▂▁▁▁▂▁▁▁▂▂▁▁▂▁▁▂</td></tr><tr><td>rmse_loss</td><td>█▇▅▄▄▃▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00513</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01805</td></tr><tr><td>Dis_B_loss</td><td>-0.00417</td></tr><tr><td>Dis_B_step_loss</td><td>-0.0046</td></tr><tr><td>Gen_loss</td><td>0.62848</td></tr><tr><td>rmse_loss</td><td>9.65646</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">semtodepth_c3</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24xy8g59\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24xy8g59</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_160909-24xy8g59/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"semtodepth_c3\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78f21a",
   "metadata": {},
   "source": [
    "# Simulation Sem to Simulation Depth Case #4 training \n",
    "## Add Guided L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f153cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_160925-3jn8jvw7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3jn8jvw7\" target=\"_blank\">semtodepth_c4</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.6604704260826111}, rmse loss - 14.101684195529051\n",
      "epoch - 1, gen loss - {'gen': 0.6534086465835571}, rmse loss - 12.544267869083644\n",
      "epoch - 2, gen loss - {'gen': 0.6616231203079224}, rmse loss - 12.251521453646276\n",
      "epoch - 3, gen loss - {'gen': 0.6316924691200256}, rmse loss - 12.500395120289932\n",
      "epoch - 4, gen loss - {'gen': 0.6813871264457703}, rmse loss - 11.685922700100718\n",
      "epoch - 5, gen loss - {'gen': 0.6801038384437561}, rmse loss - 11.540210567277295\n",
      "epoch - 6, gen loss - {'gen': 0.885643720626831}, rmse loss - 10.980432211253037\n",
      "epoch - 7, gen loss - {'gen': 0.7650645971298218}, rmse loss - 10.950064016884104\n",
      "epoch - 8, gen loss - {'gen': 0.9081915020942688}, rmse loss - 10.707524371762997\n",
      "epoch - 9, gen loss - {'gen': 0.7454079985618591}, rmse loss - 10.502712001659773\n",
      "epoch - 10, gen loss - {'gen': 0.9378820657730103}, rmse loss - 10.646602352606854\n",
      "epoch - 11, gen loss - {'gen': 0.898623526096344}, rmse loss - 10.516715310156565\n",
      "epoch - 12, gen loss - {'gen': 0.7776182293891907}, rmse loss - 10.940442486442763\n",
      "epoch - 13, gen loss - {'gen': 0.8504703044891357}, rmse loss - 10.860183112295791\n",
      "epoch - 14, gen loss - {'gen': 0.6684210896492004}, rmse loss - 10.647935897221865\n",
      "epoch - 15, gen loss - {'gen': 0.7266626358032227}, rmse loss - 10.72524908108025\n",
      "epoch - 16, gen loss - {'gen': 0.8304111361503601}, rmse loss - 11.033094803785486\n",
      "epoch - 17, gen loss - {'gen': 0.6431676149368286}, rmse loss - 10.735994009954023\n",
      "epoch - 18, gen loss - {'gen': 0.7344669699668884}, rmse loss - 10.785398562455969\n",
      "epoch - 19, gen loss - {'gen': 0.7086151242256165}, rmse loss - 10.702311051287774\n",
      "epoch - 20, gen loss - {'gen': 0.6353893280029297}, rmse loss - 10.682369214582268\n",
      "epoch - 21, gen loss - {'gen': 0.7369704246520996}, rmse loss - 10.61945764956879\n",
      "epoch - 22, gen loss - {'gen': 0.691706657409668}, rmse loss - 10.562237799387576\n",
      "epoch - 23, gen loss - {'gen': 0.7218684554100037}, rmse loss - 10.62710795631268\n",
      "epoch - 24, gen loss - {'gen': 0.5728321671485901}, rmse loss - 10.677676054824323\n",
      "epoch - 25, gen loss - {'gen': 0.39997702836990356}, rmse loss - 10.484842435900136\n",
      "epoch - 26, gen loss - {'gen': 0.5607761144638062}, rmse loss - 10.770249051801393\n",
      "epoch - 27, gen loss - {'gen': 0.6192633509635925}, rmse loss - 10.767444526137462\n",
      "epoch - 28, gen loss - {'gen': 0.4469147324562073}, rmse loss - 10.829208286046102\n",
      "epoch - 29, gen loss - {'gen': 0.5135016441345215}, rmse loss - 10.875684396807118\n",
      "epoch - 30, gen loss - {'gen': 0.4142197370529175}, rmse loss - 10.870449293143635\n",
      "epoch - 31, gen loss - {'gen': 0.3765817880630493}, rmse loss - 10.784205503569318\n",
      "epoch - 32, gen loss - {'gen': 0.46231570839881897}, rmse loss - 10.816381727197513\n",
      "epoch - 33, gen loss - {'gen': 0.4930981397628784}, rmse loss - 10.784381347388798\n",
      "epoch - 34, gen loss - {'gen': 0.4288565516471863}, rmse loss - 10.747779693110843\n",
      "epoch - 35, gen loss - {'gen': 0.3625422716140747}, rmse loss - 10.94881694519212\n",
      "epoch - 36, gen loss - {'gen': 0.3727306127548218}, rmse loss - 10.851214426469978\n",
      "epoch - 37, gen loss - {'gen': 0.41932111978530884}, rmse loss - 10.809191598223585\n",
      "epoch - 38, gen loss - {'gen': 0.4299589693546295}, rmse loss - 10.851413536775596\n",
      "epoch - 39, gen loss - {'gen': 0.3430243134498596}, rmse loss - 10.721665449248029\n",
      "epoch - 40, gen loss - {'gen': 0.3800376057624817}, rmse loss - 10.799847226301242\n",
      "epoch - 41, gen loss - {'gen': 0.45006227493286133}, rmse loss - 10.816264926727408\n",
      "epoch - 42, gen loss - {'gen': 0.44489696621894836}, rmse loss - 10.907622953182656\n",
      "epoch - 43, gen loss - {'gen': 0.43860822916030884}, rmse loss - 10.903961521233139\n",
      "epoch - 44, gen loss - {'gen': 0.4753037095069885}, rmse loss - 10.912513782177465\n",
      "epoch - 45, gen loss - {'gen': 0.42342469096183777}, rmse loss - 10.89469063677911\n",
      "epoch - 46, gen loss - {'gen': 0.39491814374923706}, rmse loss - 10.946606153931565\n",
      "epoch - 47, gen loss - {'gen': 0.4597886800765991}, rmse loss - 10.87088727775095\n",
      "epoch - 48, gen loss - {'gen': 0.4433804750442505}, rmse loss - 10.941461455778002\n",
      "epoch - 49, gen loss - {'gen': 0.4406980574131012}, rmse loss - 10.92301688423016\n",
      "training end, best epoch - 0, best valid rmse loss - 10.484842435900136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▂▇▂▄▇▁▄▃█▂▂▂▅▃▃▆▃▁▄▃▆▂▄▅▅▃▂▅▄▆▄▃▂▃▆▄▄▅▁▂</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▆▅▅▃▁▃▅▇▅▆▇▇▅▆▆█▆▆▅▆▇▆▅▆▆▆▇▆█▇▇▆▆▅▆▆▆▆▇</td></tr><tr><td>Gen_loss</td><td>█▃▃▃▃▄▄▅▅▅▄▄▄▄▄▄▃▄▄▃▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▂▂▁▁▂</td></tr><tr><td>rmse_loss</td><td>█▅▄▅▃▂▂▁▁▁▂▂▁▂▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00535</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01099</td></tr><tr><td>Dis_B_loss</td><td>-0.00454</td></tr><tr><td>Dis_B_step_loss</td><td>-0.0027</td></tr><tr><td>Gen_loss</td><td>0.44567</td></tr><tr><td>rmse_loss</td><td>10.92302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">semtodepth_c4</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3jn8jvw7\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3jn8jvw7</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_160925-3jn8jvw7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"semtodepth_c4\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'semtodepth', True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d8bb2",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #1 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692a01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_161008-3shfmk0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3shfmk0f\" target=\"_blank\">simtotrain_c1</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.749372124671936}, rmse loss - 15.811355872347786\n",
      "epoch - 1, gen loss - {'gen': 0.6989802122116089}, rmse loss - 15.31605438464682\n",
      "epoch - 2, gen loss - {'gen': 0.7218635082244873}, rmse loss - 15.507768078483778\n",
      "epoch - 3, gen loss - {'gen': 0.5630615949630737}, rmse loss - 14.839394440950063\n",
      "epoch - 4, gen loss - {'gen': 0.6045677661895752}, rmse loss - 15.071120026366737\n",
      "epoch - 5, gen loss - {'gen': 0.6629830598831177}, rmse loss - 15.0545671399669\n",
      "epoch - 6, gen loss - {'gen': 0.590895414352417}, rmse loss - 14.70248695669139\n",
      "epoch - 7, gen loss - {'gen': 0.5510333180427551}, rmse loss - 14.766395415767093\n",
      "epoch - 8, gen loss - {'gen': 0.5571500062942505}, rmse loss - 14.699557522566117\n",
      "epoch - 9, gen loss - {'gen': 0.5645549893379211}, rmse loss - 14.764706067933368\n",
      "epoch - 10, gen loss - {'gen': 0.5596492290496826}, rmse loss - 15.017865675401863\n",
      "epoch - 11, gen loss - {'gen': 0.5395370125770569}, rmse loss - 14.82194019507658\n",
      "epoch - 12, gen loss - {'gen': 0.4865556061267853}, rmse loss - 14.880158283613705\n",
      "epoch - 13, gen loss - {'gen': 0.4739795923233032}, rmse loss - 15.036749945355517\n",
      "epoch - 14, gen loss - {'gen': 0.36420005559921265}, rmse loss - 14.883076168074378\n",
      "epoch - 15, gen loss - {'gen': 0.3386988043785095}, rmse loss - 14.824229530742688\n",
      "epoch - 16, gen loss - {'gen': 0.36749815940856934}, rmse loss - 14.794197547039863\n",
      "epoch - 17, gen loss - {'gen': 0.3019868731498718}, rmse loss - 15.003995304178048\n",
      "epoch - 18, gen loss - {'gen': 0.3547714650630951}, rmse loss - 14.964907968176247\n",
      "epoch - 19, gen loss - {'gen': 0.27641457319259644}, rmse loss - 14.865411260471133\n",
      "epoch - 20, gen loss - {'gen': 0.2782076597213745}, rmse loss - 15.029710338564376\n",
      "epoch - 21, gen loss - {'gen': 0.3259853720664978}, rmse loss - 15.14710562343527\n",
      "epoch - 22, gen loss - {'gen': 0.2958032488822937}, rmse loss - 15.150879856405224\n",
      "epoch - 23, gen loss - {'gen': 0.22086071968078613}, rmse loss - 14.96864308290376\n",
      "epoch - 24, gen loss - {'gen': 0.38651183247566223}, rmse loss - 15.103410852791198\n",
      "epoch - 25, gen loss - {'gen': 0.26500916481018066}, rmse loss - 15.002513271416245\n",
      "epoch - 26, gen loss - {'gen': 0.37962788343429565}, rmse loss - 15.102497315494777\n",
      "epoch - 27, gen loss - {'gen': 0.3400012254714966}, rmse loss - 15.005086592642584\n",
      "epoch - 28, gen loss - {'gen': 0.23390649259090424}, rmse loss - 14.969684197893882\n",
      "epoch - 29, gen loss - {'gen': 0.24203118681907654}, rmse loss - 15.12186976992336\n",
      "epoch - 30, gen loss - {'gen': 0.25208568572998047}, rmse loss - 15.075888725224456\n",
      "epoch - 31, gen loss - {'gen': 0.1941286027431488}, rmse loss - 15.060505212453018\n",
      "epoch - 32, gen loss - {'gen': 0.1979738175868988}, rmse loss - 15.037645255507579\n",
      "epoch - 33, gen loss - {'gen': 0.1839601695537567}, rmse loss - 15.11795093388575\n",
      "epoch - 34, gen loss - {'gen': 0.17159244418144226}, rmse loss - 15.084293518559079\n",
      "epoch - 35, gen loss - {'gen': 0.21348941326141357}, rmse loss - 15.111613004409959\n",
      "epoch - 36, gen loss - {'gen': 0.22468432784080505}, rmse loss - 15.035516350031779\n",
      "epoch - 37, gen loss - {'gen': 0.21871018409729004}, rmse loss - 14.977629156570153\n",
      "epoch - 38, gen loss - {'gen': 0.23211295902729034}, rmse loss - 15.134681589049167\n",
      "epoch - 39, gen loss - {'gen': 0.23312699794769287}, rmse loss - 15.057419581606819\n",
      "epoch - 40, gen loss - {'gen': 0.17429611086845398}, rmse loss - 15.09892228960551\n",
      "epoch - 41, gen loss - {'gen': 0.22965136170387268}, rmse loss - 14.959200788687955\n",
      "epoch - 42, gen loss - {'gen': 0.17269465327262878}, rmse loss - 15.024217551044872\n",
      "epoch - 43, gen loss - {'gen': 0.19864735007286072}, rmse loss - 15.067620112007395\n",
      "epoch - 44, gen loss - {'gen': 0.2470892071723938}, rmse loss - 15.035519489063109\n",
      "epoch - 45, gen loss - {'gen': 0.1450091004371643}, rmse loss - 15.067333108824558\n",
      "epoch - 46, gen loss - {'gen': 0.1727803349494934}, rmse loss - 15.06510396373228\n",
      "epoch - 47, gen loss - {'gen': 0.15160170197486877}, rmse loss - 15.075459967680082\n",
      "epoch - 48, gen loss - {'gen': 0.16076427698135376}, rmse loss - 15.078053164746049\n",
      "epoch - 49, gen loss - {'gen': 0.1878555417060852}, rmse loss - 15.0917213763698\n",
      "training end, best epoch - 0, best valid rmse loss - 14.699557522566117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▆▅▅▄▄▄▄▆▄▃▃▄▃▅▃▂▂█▅▃▃▂▃▁▅▂▅▅▅▅▃▃▄▂▃▂▃▂▄</td></tr><tr><td>Dis_B_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>▆█▆▁▁▃▄▄▅▂▅▂▆▄▄▆▄▄▆▆▅▅▅▄▅▄▅▅▅▅▅▆▆▅▄▄▄▆▅▆</td></tr><tr><td>Gen_loss</td><td>█▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▅▆▂▃▁▁▁▃▂▂▃▂▂▃▃▃▄▄▃▃▄▃▃▃▃▃▄▄▃▃▄▄▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.0156</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01445</td></tr><tr><td>Dis_B_loss</td><td>-0.01106</td></tr><tr><td>Dis_B_step_loss</td><td>-0.01438</td></tr><tr><td>Gen_loss</td><td>0.18935</td></tr><tr><td>rmse_loss</td><td>15.09172</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">simtotrain_c1</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3shfmk0f\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/3shfmk0f</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_161008-3shfmk0f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"simtotrain_c1\")\n",
    "training(1, cfg['epochs'], cfg['device'], 'simtotrain', False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99cd5c",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #2 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "632d4a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_161032-2f3gyre9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2f3gyre9\" target=\"_blank\">simtotrain_c2</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.8475733995437622}, rmse loss - 16.544937251678693\n",
      "epoch - 1, gen loss - {'gen': 0.873705267906189}, rmse loss - 16.31470018647254\n",
      "epoch - 2, gen loss - {'gen': 0.9549434185028076}, rmse loss - 16.375406379629325\n",
      "epoch - 3, gen loss - {'gen': 0.8656570315361023}, rmse loss - 16.41269726946785\n",
      "epoch - 4, gen loss - {'gen': 0.7125281095504761}, rmse loss - 15.269447402320665\n",
      "epoch - 5, gen loss - {'gen': 0.609251856803894}, rmse loss - 15.164793614531796\n",
      "epoch - 6, gen loss - {'gen': 0.6530883312225342}, rmse loss - 14.655090168393405\n",
      "epoch - 7, gen loss - {'gen': 0.591376543045044}, rmse loss - 14.433143455603906\n",
      "epoch - 8, gen loss - {'gen': 0.5347281694412231}, rmse loss - 14.369638400764043\n",
      "epoch - 9, gen loss - {'gen': 0.4520851671695709}, rmse loss - 14.129558561472875\n",
      "epoch - 10, gen loss - {'gen': 0.48283660411834717}, rmse loss - 13.96830759013271\n",
      "epoch - 11, gen loss - {'gen': 0.5279279947280884}, rmse loss - 13.871883376498063\n",
      "epoch - 12, gen loss - {'gen': 0.5430178642272949}, rmse loss - 13.886866648698645\n",
      "epoch - 13, gen loss - {'gen': 0.46945932507514954}, rmse loss - 13.785661885659193\n",
      "epoch - 14, gen loss - {'gen': 0.46769142150878906}, rmse loss - 13.86326385160214\n",
      "epoch - 15, gen loss - {'gen': 0.46952933073043823}, rmse loss - 13.632440943559597\n",
      "epoch - 16, gen loss - {'gen': 0.40792548656463623}, rmse loss - 13.514696759931276\n",
      "epoch - 17, gen loss - {'gen': 0.4084163308143616}, rmse loss - 13.669661337159217\n",
      "epoch - 18, gen loss - {'gen': 0.42986172437667847}, rmse loss - 13.727570530233349\n",
      "epoch - 19, gen loss - {'gen': 0.5283267498016357}, rmse loss - 13.652177868733986\n",
      "epoch - 20, gen loss - {'gen': 0.30922815203666687}, rmse loss - 13.798204543405793\n",
      "epoch - 21, gen loss - {'gen': 0.4460027813911438}, rmse loss - 13.890455599640568\n",
      "epoch - 22, gen loss - {'gen': 0.3803401589393616}, rmse loss - 13.88039373207796\n",
      "epoch - 23, gen loss - {'gen': 0.3448452651500702}, rmse loss - 13.83864069688804\n",
      "epoch - 24, gen loss - {'gen': 0.46864593029022217}, rmse loss - 13.90441230478322\n",
      "epoch - 25, gen loss - {'gen': 0.34756213426589966}, rmse loss - 13.96436223244755\n",
      "epoch - 26, gen loss - {'gen': 0.4762779474258423}, rmse loss - 13.849097895886185\n",
      "epoch - 27, gen loss - {'gen': 0.4541967511177063}, rmse loss - 13.92666927210959\n",
      "epoch - 28, gen loss - {'gen': 0.487223744392395}, rmse loss - 14.038335728029484\n",
      "epoch - 29, gen loss - {'gen': 0.40256467461586}, rmse loss - 13.969305941099611\n",
      "epoch - 30, gen loss - {'gen': 0.33675187826156616}, rmse loss - 13.954683882723875\n",
      "epoch - 31, gen loss - {'gen': 0.3397153913974762}, rmse loss - 14.077049146279197\n",
      "epoch - 32, gen loss - {'gen': 0.36453622579574585}, rmse loss - 14.070986330729129\n",
      "epoch - 33, gen loss - {'gen': 0.49103283882141113}, rmse loss - 13.91242613739633\n",
      "epoch - 34, gen loss - {'gen': 0.3886542022228241}, rmse loss - 14.147048253414816\n",
      "epoch - 35, gen loss - {'gen': 0.38232994079589844}, rmse loss - 14.231740523968236\n",
      "epoch - 36, gen loss - {'gen': 0.36124682426452637}, rmse loss - 14.2516720901996\n",
      "epoch - 37, gen loss - {'gen': 0.3926530182361603}, rmse loss - 14.19979350769212\n",
      "epoch - 38, gen loss - {'gen': 0.4021715521812439}, rmse loss - 14.210741400278803\n",
      "epoch - 39, gen loss - {'gen': 0.38973891735076904}, rmse loss - 14.2499810180101\n",
      "epoch - 40, gen loss - {'gen': 0.3684609532356262}, rmse loss - 14.284269621451402\n",
      "epoch - 41, gen loss - {'gen': 0.3555879592895508}, rmse loss - 14.319640184240587\n",
      "epoch - 42, gen loss - {'gen': 0.38815808296203613}, rmse loss - 14.278983119668995\n",
      "epoch - 43, gen loss - {'gen': 0.3389943838119507}, rmse loss - 14.246079103533193\n",
      "epoch - 44, gen loss - {'gen': 0.39422160387039185}, rmse loss - 14.276687432039267\n",
      "epoch - 45, gen loss - {'gen': 0.37633368372917175}, rmse loss - 14.301714444952257\n",
      "epoch - 46, gen loss - {'gen': 0.3322295546531677}, rmse loss - 14.356811884144575\n",
      "epoch - 47, gen loss - {'gen': 0.3606216311454773}, rmse loss - 14.363436521199356\n",
      "epoch - 48, gen loss - {'gen': 0.32916441559791565}, rmse loss - 14.300214971563474\n",
      "epoch - 49, gen loss - {'gen': 0.32266873121261597}, rmse loss - 14.367495468181877\n",
      "training end, best epoch - 0, best valid rmse loss - 13.514696759931276\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▄▁▆▂▅▅▆▃▆▅▅▇▅▅▄▇▂▄▅▇▅▄▅▅▄▅▆▄▅█▄▆▄▅▄▄▆▃▅</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>▇▆▇▅▃▂▁▃█▇▆▆▆▆▇▅▆▆▇▆▅▆▇▇▆▇▇█▇▇▇▅▆▆▇▅▇▆▆▆</td></tr><tr><td>Gen_loss</td><td>█▄▅▅▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▇██▅▄▃▃▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00601</td></tr><tr><td>Dis_A_step_loss</td><td>0.00856</td></tr><tr><td>Dis_B_loss</td><td>-0.00981</td></tr><tr><td>Dis_B_step_loss</td><td>-0.00129</td></tr><tr><td>Gen_loss</td><td>0.34611</td></tr><tr><td>rmse_loss</td><td>14.3675</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">simtotrain_c2</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2f3gyre9\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2f3gyre9</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_161032-2f3gyre9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"simtotrain_c2\")\n",
    "training(2, cfg['epochs'], cfg['device'], 'simtotrain', False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc1a1a",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #3 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c277efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_161049-24prruiu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24prruiu\" target=\"_blank\">simtotrain_c3</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.7993854284286499}, rmse loss - 16.955419677649918\n",
      "epoch - 1, gen loss - {'gen': 0.7363439202308655}, rmse loss - 16.20881504326289\n",
      "epoch - 2, gen loss - {'gen': 0.7477864623069763}, rmse loss - 16.169494326264218\n",
      "epoch - 3, gen loss - {'gen': 0.8498913049697876}, rmse loss - 16.049047179767566\n",
      "epoch - 4, gen loss - {'gen': 0.6017487049102783}, rmse loss - 15.561492033110333\n",
      "epoch - 5, gen loss - {'gen': 0.540407657623291}, rmse loss - 15.44299863977186\n",
      "epoch - 6, gen loss - {'gen': 0.5702083706855774}, rmse loss - 15.686407307417191\n",
      "epoch - 7, gen loss - {'gen': 0.4010618329048157}, rmse loss - 16.40861315041011\n",
      "epoch - 8, gen loss - {'gen': 0.2969590127468109}, rmse loss - 15.229439114292608\n",
      "epoch - 9, gen loss - {'gen': 0.3303644061088562}, rmse loss - 14.883136615542028\n",
      "epoch - 10, gen loss - {'gen': 0.23134900629520416}, rmse loss - 14.81949131427216\n",
      "epoch - 11, gen loss - {'gen': 0.3446091413497925}, rmse loss - 15.070698187360025\n",
      "epoch - 12, gen loss - {'gen': 0.3122757077217102}, rmse loss - 14.826602172147744\n",
      "epoch - 13, gen loss - {'gen': 0.28886014223098755}, rmse loss - 14.922970238646897\n",
      "epoch - 14, gen loss - {'gen': 0.2872323989868164}, rmse loss - 14.87973320000286\n",
      "epoch - 15, gen loss - {'gen': 0.3117372393608093}, rmse loss - 15.02716340877913\n",
      "epoch - 16, gen loss - {'gen': 0.28688985109329224}, rmse loss - 14.856861719786021\n",
      "epoch - 17, gen loss - {'gen': 0.3559049665927887}, rmse loss - 15.283567615100818\n",
      "epoch - 18, gen loss - {'gen': 0.3391863703727722}, rmse loss - 15.58266103751545\n",
      "epoch - 19, gen loss - {'gen': 0.2744995653629303}, rmse loss - 15.442522411416817\n",
      "epoch - 20, gen loss - {'gen': 0.2757498025894165}, rmse loss - 15.858565976259014\n",
      "epoch - 21, gen loss - {'gen': 0.11623044312000275}, rmse loss - 15.674264082609508\n",
      "epoch - 22, gen loss - {'gen': 0.252908319234848}, rmse loss - 15.745379840315929\n",
      "epoch - 23, gen loss - {'gen': 0.2380073219537735}, rmse loss - 15.754451853762694\n",
      "epoch - 24, gen loss - {'gen': 0.24932053685188293}, rmse loss - 15.966998121395322\n",
      "epoch - 25, gen loss - {'gen': 0.24310043454170227}, rmse loss - 16.14939436437459\n",
      "epoch - 26, gen loss - {'gen': 0.3501015305519104}, rmse loss - 16.040289248927493\n",
      "epoch - 27, gen loss - {'gen': 0.20172752439975739}, rmse loss - 15.825264869140963\n",
      "epoch - 28, gen loss - {'gen': 0.231159046292305}, rmse loss - 16.104960068565454\n",
      "epoch - 29, gen loss - {'gen': 0.24381141364574432}, rmse loss - 16.047968864440918\n",
      "epoch - 30, gen loss - {'gen': 0.20799189805984497}, rmse loss - 15.991351926458718\n",
      "epoch - 31, gen loss - {'gen': 0.13136936724185944}, rmse loss - 16.17650896156846\n",
      "epoch - 32, gen loss - {'gen': 0.1784379482269287}, rmse loss - 16.034087862035886\n",
      "epoch - 33, gen loss - {'gen': 0.11417607218027115}, rmse loss - 16.183387277750953\n",
      "epoch - 34, gen loss - {'gen': 0.15409718453884125}, rmse loss - 16.132646470932062\n",
      "epoch - 35, gen loss - {'gen': 0.2002878338098526}, rmse loss - 16.128910476431194\n",
      "epoch - 36, gen loss - {'gen': 0.14393070340156555}, rmse loss - 16.23588965475779\n",
      "epoch - 37, gen loss - {'gen': 0.18279224634170532}, rmse loss - 16.369248872313552\n",
      "epoch - 38, gen loss - {'gen': 0.18858686089515686}, rmse loss - 16.203125040469573\n",
      "epoch - 39, gen loss - {'gen': 0.1822686791419983}, rmse loss - 16.176378155106548\n",
      "epoch - 40, gen loss - {'gen': 0.1863517463207245}, rmse loss - 16.10902952356092\n",
      "epoch - 41, gen loss - {'gen': 0.2469358742237091}, rmse loss - 16.108004772355194\n",
      "epoch - 42, gen loss - {'gen': 0.19823575019836426}, rmse loss - 16.165060861524182\n",
      "epoch - 43, gen loss - {'gen': 0.20135267078876495}, rmse loss - 16.01811791873946\n",
      "epoch - 44, gen loss - {'gen': 0.14044511318206787}, rmse loss - 16.180481984606526\n",
      "epoch - 45, gen loss - {'gen': 0.15207357704639435}, rmse loss - 16.10304045413253\n",
      "epoch - 46, gen loss - {'gen': 0.13225950300693512}, rmse loss - 16.055405562214307\n",
      "epoch - 47, gen loss - {'gen': 0.05718254670500755}, rmse loss - 16.044061687159804\n",
      "epoch - 48, gen loss - {'gen': 0.14072537422180176}, rmse loss - 15.993521495058967\n",
      "epoch - 49, gen loss - {'gen': 0.11290125548839569}, rmse loss - 16.059486090037215\n",
      "training end, best epoch - 0, best valid rmse loss - 14.81949131427216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>▅▂▁▅▄▅▆▄▄▇▄▃▄▄▂▅▅█▄▆▅▄▃▅▄▄▃▃▃▄▅▂▃▅▄▃▃▄▂▄</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>█▇▇▄▃▂▁▅▂▄▆▆▆▅▇▇▆▆▆▇▇▅▆▇▅▅▅▆▅▄▇▇▇▆▆▇▆▅▅▆</td></tr><tr><td>Gen_loss</td><td>█▆▅▆▄▄▄▃▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▁▂▂▂▁▁▁▁</td></tr><tr><td>rmse_loss</td><td>█▆▅▅▃▄▆▂▁▂▁▁▂▁▃▄▄▄▄▄▅▅▄▅▅▅▅▅▅▆▆▆▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.00655</td></tr><tr><td>Dis_A_step_loss</td><td>-0.01271</td></tr><tr><td>Dis_B_loss</td><td>-0.01161</td></tr><tr><td>Dis_B_step_loss</td><td>-0.00589</td></tr><tr><td>Gen_loss</td><td>0.1496</td></tr><tr><td>rmse_loss</td><td>16.05949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">simtotrain_c3</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24prruiu\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/24prruiu</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_161049-24prruiu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"simtotrain_c3\")\n",
    "training(3, cfg['epochs'], cfg['device'], 'simtotrain', False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1cb94",
   "metadata": {},
   "source": [
    "# Simulation Sem to Train Sem Case #4 training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc5d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimjiil2013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kji/workspace/jupyter_kji/wandb/run-20221227_161106-2sh76znc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2sh76znc\" target=\"_blank\">simtotrain_c4</a></strong> to <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, gen loss - {'gen': 0.6394338607788086}, rmse loss - 14.568509172249545\n",
      "epoch - 1, gen loss - {'gen': 0.570279598236084}, rmse loss - 14.762874283033984\n",
      "epoch - 2, gen loss - {'gen': 0.5614814758300781}, rmse loss - 14.92353095339673\n",
      "epoch - 3, gen loss - {'gen': 0.5641460418701172}, rmse loss - 14.836537565252438\n",
      "epoch - 4, gen loss - {'gen': 0.5018516778945923}, rmse loss - 15.050771956074282\n",
      "epoch - 5, gen loss - {'gen': 0.6536893844604492}, rmse loss - 14.852180676266716\n",
      "epoch - 6, gen loss - {'gen': 0.6067661046981812}, rmse loss - 14.672394043405118\n",
      "epoch - 7, gen loss - {'gen': 0.41985756158828735}, rmse loss - 14.858069326604864\n",
      "epoch - 8, gen loss - {'gen': 0.14360401034355164}, rmse loss - 14.804652662734703\n",
      "epoch - 9, gen loss - {'gen': 0.15292397141456604}, rmse loss - 14.822184381449794\n",
      "epoch - 10, gen loss - {'gen': 0.3205029368400574}, rmse loss - 14.711959705141638\n",
      "epoch - 11, gen loss - {'gen': 0.2846326231956482}, rmse loss - 14.691237972231368\n",
      "epoch - 12, gen loss - {'gen': 0.28453487157821655}, rmse loss - 14.79633204083601\n",
      "epoch - 13, gen loss - {'gen': 0.23402880132198334}, rmse loss - 14.843314501632184\n",
      "epoch - 14, gen loss - {'gen': 0.29084089398384094}, rmse loss - 14.822086314873502\n",
      "epoch - 15, gen loss - {'gen': 0.29448628425598145}, rmse loss - 14.766311455476767\n",
      "epoch - 16, gen loss - {'gen': 0.16769173741340637}, rmse loss - 14.911818949498814\n",
      "epoch - 17, gen loss - {'gen': 0.08667121827602386}, rmse loss - 15.010851092884021\n",
      "epoch - 18, gen loss - {'gen': 0.20765700936317444}, rmse loss - 15.002025428293376\n",
      "epoch - 19, gen loss - {'gen': 0.16941925883293152}, rmse loss - 15.27291626595923\n",
      "epoch - 20, gen loss - {'gen': 0.2804291844367981}, rmse loss - 15.28418506115565\n",
      "epoch - 21, gen loss - {'gen': 0.11359281092882156}, rmse loss - 15.322827983166459\n",
      "epoch - 22, gen loss - {'gen': 0.19902628660202026}, rmse loss - 15.202937416484875\n",
      "epoch - 23, gen loss - {'gen': 0.018669486045837402}, rmse loss - 15.56582524767661\n",
      "epoch - 24, gen loss - {'gen': 0.028720282018184662}, rmse loss - 15.321157429051135\n",
      "epoch - 25, gen loss - {'gen': -0.13253355026245117}, rmse loss - 15.32858408157236\n",
      "epoch - 26, gen loss - {'gen': 0.08136427402496338}, rmse loss - 15.163815491313864\n",
      "epoch - 27, gen loss - {'gen': -0.04150404781103134}, rmse loss - 15.212912209359482\n",
      "epoch - 28, gen loss - {'gen': 0.08845679461956024}, rmse loss - 15.238894107157014\n",
      "epoch - 29, gen loss - {'gen': -0.04024747014045715}, rmse loss - 15.230563985465638\n",
      "epoch - 30, gen loss - {'gen': 0.017053302377462387}, rmse loss - 15.068936947966854\n",
      "epoch - 31, gen loss - {'gen': 0.014555417001247406}, rmse loss - 15.319455296351022\n",
      "epoch - 32, gen loss - {'gen': -0.04082242399454117}, rmse loss - 15.321170410986756\n",
      "epoch - 33, gen loss - {'gen': -0.07067909836769104}, rmse loss - 15.188639192123695\n",
      "epoch - 34, gen loss - {'gen': -0.010074354708194733}, rmse loss - 15.169141227468794\n",
      "epoch - 35, gen loss - {'gen': 0.15280640125274658}, rmse loss - 15.19857204444294\n",
      "epoch - 36, gen loss - {'gen': 0.12436835467815399}, rmse loss - 15.454521480081706\n",
      "epoch - 37, gen loss - {'gen': 0.050722524523735046}, rmse loss - 15.269707813474085\n",
      "epoch - 38, gen loss - {'gen': 0.04426245391368866}, rmse loss - 15.192414179939185\n",
      "epoch - 39, gen loss - {'gen': 0.1066029742360115}, rmse loss - 15.371374336115988\n",
      "epoch - 40, gen loss - {'gen': 0.11685530841350555}, rmse loss - 15.33228994559538\n",
      "epoch - 41, gen loss - {'gen': 0.06325015425682068}, rmse loss - 15.370662664575331\n",
      "epoch - 42, gen loss - {'gen': 0.1091991662979126}, rmse loss - 15.22463696997104\n",
      "epoch - 43, gen loss - {'gen': 0.04514692723751068}, rmse loss - 15.324270989182251\n",
      "epoch - 44, gen loss - {'gen': 0.09415145218372345}, rmse loss - 15.210103860200551\n",
      "epoch - 45, gen loss - {'gen': 0.06782656162977219}, rmse loss - 15.283010194222426\n",
      "epoch - 46, gen loss - {'gen': 0.04395631328225136}, rmse loss - 15.282840100601591\n",
      "epoch - 47, gen loss - {'gen': 0.054944925010204315}, rmse loss - 15.290226711118354\n",
      "epoch - 48, gen loss - {'gen': 0.04012615978717804}, rmse loss - 15.299533680356296\n",
      "epoch - 49, gen loss - {'gen': 0.06638198345899582}, rmse loss - 15.235400678486842\n",
      "training end, best epoch - 0, best valid rmse loss - 14.568509172249545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_A_step_loss</td><td>█▆▆▃▆▄▆▁▃▄▂▂▆▆▅▃▃▃▃▆▃▃▇▅▄▁▄▃▄▅▆▅▄▂▄▃▄▄▅▅</td></tr><tr><td>Dis_B_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Dis_B_step_loss</td><td>██▆▅▃▁▃▆▆▅▅▄▇▆█▄▂▅▅▆▄▅▄▅▆▄▆▆▇▆▅▅▆▅▆▆▆▆▄▄</td></tr><tr><td>Gen_loss</td><td>█▅▄▄▄▅▄▃▂▃▃▂▃▃▂▂▂▂▂▂▁▁▂▂▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>rmse_loss</td><td>▁▂▃▃▃▂▃▃▂▂▃▃▂▃▄▄▆▆▅█▆▅▆▆▅▆▆▅▅▇▆▅▆▇▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dis_A_loss</td><td>-0.01058</td></tr><tr><td>Dis_A_step_loss</td><td>-0.02343</td></tr><tr><td>Dis_B_loss</td><td>-0.00822</td></tr><tr><td>Dis_B_step_loss</td><td>-0.00601</td></tr><tr><td>Gen_loss</td><td>0.05908</td></tr><tr><td>rmse_loss</td><td>15.2354</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">simtotrain_c4</strong>: <a href=\"https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2sh76znc\" target=\"_blank\">https://wandb.ai/kimjiil2013/221227_Samsung_sem/runs/2sh76znc</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221227_161106-2sh76znc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, entity=\"kimjiil2013\", name=\"simtotrain_c4\")\n",
    "training(4, cfg['epochs'], cfg['device'], 'simtotrain', False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3039c",
   "metadata": {},
   "source": [
    "# Submission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becf7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_dataset(Dataset):\n",
    "    def __init__(self, path_list, transforms):\n",
    "        super(test_dataset, self).__init__()\n",
    "        self.path_list = path_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.path_list[idx]\n",
    "        img = None\n",
    "        depth = 0\n",
    "        img_name = None\n",
    "\n",
    "        if isinstance(item, str):\n",
    "            img = PIL.Image.open(item).convert(\"L\")\n",
    "            img_name = item.split('/')[-1].split('\\\\')[-1]\n",
    "        elif isinstance(item, list):\n",
    "            img = PIL.Image.open(item[0]).convert(\"L\")\n",
    "            img_name = item[0].split('/')[-1].split('\\\\')[-1]\n",
    "            depth = item[1]\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        img = np.array(img).astype(np.float32) / 255.\n",
    "        img = img.reshape(1, *img.shape)\n",
    "\n",
    "        return img, depth, img_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27fd6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data_path_list = glob.glob(os.path.join(cfg['db_path'], \"sample_submission\",  '*.png'))\n",
    "submission_db = test_dataset(submission_data_path_list, None)\n",
    "submission_dataloader = DataLoader(submission_db, batch_size=cfg['batch_size'], num_workers=cfg['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8ec0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/010902.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/016640.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/006260.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/013556.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/006584.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/022097.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/023371.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/009801.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/001504.png',\n",
       " '/home/kji/workspace/jupyter_kji/samsumg_sem_dataset/sample_submission/019025.png']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data_path_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63745b8c",
   "metadata": {},
   "source": [
    "# CNN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3735c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        mobv3s = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "        feature = [nn.Sequential(nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2,2), padding=(1,1), bias=False),\n",
    "                   nn.BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
    "                   nn.Hardswish())]\n",
    "        feature.extend([mobv3s.features._modules[module_key] for i, module_key in enumerate(mobv3s.features._modules.keys()) if i > 0])\n",
    "\n",
    "        self.feature = nn.Sequential(*feature)\n",
    "        self.avgpool = mobv3s.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=576, out_features=128, bias=True),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=128, out_features=4, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be313ca",
   "metadata": {},
   "source": [
    "# Submission Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051080ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ganmodels(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(ganmodels, self).__init__()\n",
    "        self.models = nn.ModuleList([nn.ModuleDict({\"semtodepth\": cycleGAN_model(1),\n",
    "                                                    \"simtotrain\": cycleGAN_model(1)}) for i in range(4)])\n",
    "\n",
    "        for i in range(4):\n",
    "            for key in self.models[i]:\n",
    "                self.models[i][key].model_load(f'./case{i+1}_t({key})_best_model.pth', device)\n",
    "\n",
    "\n",
    "    def forward(self, img, cls_idx):\n",
    "        # Train SEN Image To Simulation SEM Image\n",
    "        img = self.models[cls_idx]['simtotrain'].Gen['B'](img)\n",
    "        # Simulation SEM Image to Simulation Depth Image\n",
    "        img = self.models[cls_idx]['semtodepth'].Gen['A'](img)\n",
    "\n",
    "        return img\n",
    "    \n",
    "def submission_test(datalodaer, device):\n",
    "    cls_model = torch.load(\"./best_cnn_classifer.pth\", map_location=device)\n",
    "    cls_model.eval()\n",
    "\n",
    "    ganmodel = ganmodels(device)\n",
    "    ganmodel.to(device)\n",
    "    ganmodel.eval()\n",
    "\n",
    "    for i, item in enumerate(datalodaer):\n",
    "        sem_imgs = item[0].to(device)\n",
    "        depths = item[1].to(device)\n",
    "        img_names = item[2]\n",
    "        \n",
    "        # classification SEM Image case\n",
    "        pred_cls = torch.argmax(cls_model(sem_imgs), dim=1)\n",
    "        \n",
    "        for img, cls_idx, name in zip(sem_imgs, pred_cls, img_names):\n",
    "            pred_depth = 140 + cls_idx.item() * 10\n",
    "            depth_img = ganmodel(img.reshape(1, *img.shape), cls_idx)\n",
    "            \n",
    "            # Depth Image 후처리 과정\n",
    "            depth_img_uint8 = (depth_img * 255).type(torch.uint8).detach().cpu().numpy()\n",
    "            mask = (depth_img_uint8 >= (pred_depth-1)).astype(np.float)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "            d_mask = cv2.morphologyEx(mask[0][0], cv2.MORPH_CLOSE, kernel).reshape(*mask.shape)\n",
    "            depth_img_uint8[d_mask.astype(np.bool_)] = pred_depth\n",
    "\n",
    "            os.makedirs('./submission_pred', exist_ok=True)\n",
    "            cv2.imwrite(os.path.join('./submission_pred', name), depth_img_uint8[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c019f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87183/3465738284.py:42: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = (depth_img_uint8 >= (pred_depth-1)).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "submission_test(submission_dataloader, cfg['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f9e0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAABICAAAAACVGCn3AAAFgklEQVRIDZXB2WKbOhQF0H00AMYDnmIn/6wPrYfEA9ggCelcA0mb3PbFa5HBc8jgOWTwHDJ4Dhk8hwyeQwb/FgX+iQz+EiMzgRT+hQx+ChG2jSJCKqE0/kIGP7Su9s4GYlKJTJNUCfxEBt+Eti7ru71HEMlEj0Z5nmn8QAZ/RFeV16pqfAsIoZNkPJvMRim+I4PfQnMrz5eqsSEyC5Ja5+PZcppn+IYMvkRbnc6X0toQIhhCSJVm02I5z1P8QQafQludrqfy7l2MEQSQkDrNZovlLE/xGxl8cvX5/VzWzgaODAJIkFSjfLpazFONL2QwaOvrx3vZ1C7EyHggkJBSZ5PpajnN8YUMBvZ6er/cGxt9ZDCIAUFCqiQbr5fzqcInMuiF+vR+udTOtTGC0SMSpJI0n67XS41PZNAL5ce+vNXOhxiJGR2CEFIl+Xj5shrjExn03PnjeGms84HBDDA6kqTSab5cr2YKAzLoRHs6nErb+DZGBhgMMEGQlDqZzFbbQmNABp3YfPy63Gvn2xABBhgdIil1khUv2yLFgAx61fvucrfOtTEyGA8MApGQOsmmq7f5CAMy6MTbcX+5Oec9RzADYICYSEiVprPFdp1jQAad9n7cXe/O+jYygwEwQAAJqZJsvHh9GWNABp1QHnfXpnE+xMh4YHSISGqdThZvy4lAjww64bo/XJvGuxCZwegRQEJonUzmb6sCAzLotNf9obw751uOAKNHAAmhkjSfb7ZTgR4ZdFy531e1s22IzACjQwAJKXWaF6+bmUCPDB5iOO8PZWO9jzECYAyISCqd5MXbeqbQI4OOO++OpbXeh8h4YGKAABJCaZ0Xry+FEOiQQceed8eqcS60zADjCwkhdToqNtuZRo8MOvay21fWuRCYwQAYPRJC6nRUbLdTKdAhg44/7Q5l43wbmQEGGA9MRFJpnRevLzONHhl03Hl/KGvv2xgZxAyA8UBCKJ3kxetLQQIdMuj4y69D2TjfBsYDg8EAE5GUSZoXm+1Eo0cGHXfd7ava+RCYATAYjI6QMknHxetmItEjg44/7w6ltT7EyACDwQADJKTSyXjxup4J9Mig4867Y2md9zECYAaD8UAkVZKOF2+rAgMy6Ljzr2PVON/GyMTMYDAeSEiVpvlys5kI9MigYy+7Q2mdC4EZYAaD8UBCqmQ0mb9uRgI9Muj40+5QNda3MTITg8HokFAqSWfL1+UEAzLouPPuUNXOtyEyGACjJ0ipNJuttsscAzLouPP+UDbWtSEyGL+RlDobTdabRY4BGXTc5bC/Nta1ITIYX4iE0um4WLwuEgzIoOMv+8O1sd4FjgyACUwACaGTZLRabSYpBmTQ8eXhcL1b50NkZoAJYCJIqXWWL1abcYIBGXT87X1/qRvb+sgMxoBIKq2zYr5cjqVAjww6bf1xON9q532MYDB6REomWVKsi3UmMSCDTrQfx1PVNK5tOQIMgAkklUzS6WS9mmqFARl0oi+Pl8vNWhdiBDMAJgiplZpMFuvpOMUnMui19/PHqWqs8zFGAExMJIRW2WS2Ws2UFBiQQS+66+lyrmvbtjEyA2AiIXWWTbaLIkkkPpHBoK2r0/lS175tI0eAWECLNJktF/O5UPhCBp9cU32cqrr2PkQGg4SUKhkv5i/5WOI3MvgUfXM9Xcub9aHlCEiSOhkt5vPpOMEfZPAlOnu9lNd708Y2AqRkMp7N5+NJim/I4I/GldX1eg/OM0vOVDZdTKejBN+RwTfW1eX5FqxjkFL5aDYeTUjhOzL4LvjbvWqC90JLPcpzlSmBH8jgh7ZtQgUrWMos06Q0/ocMfooM64RohcqCFPgLGfxfkIgtSQj8Cxk8hwyeQwbPIYPnkMFzyOA5ZPAcMnjOf3O5E9Z2GQaHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAABICAAAAACVGCn3AAAFnUlEQVRIDZXB6XravBoF0L0lDwwhQIAQAr1t3eDXTBgCCQYjW9J7jCFt2j7nB2vR4Do0uA4NrkOD69DgOjS4Dg2uQ4Pr0OAfXlwQQkdU+AcN/lKVtqycUMVpEicKf6HBH3yxz4vCOqeiVtpt95JE4Q80+MaXx4/t5+5YVqJ03OreDvvdROM7Gvzmbb553+SHo3eiVBR3bobj4U1L4xsa/OLK7Xa13hXWeQlU1HF7MBqP+6nCbzT44g+fm+XmoyhdkECQSsft2/HDpNfCbzS4cHa3ztbbQ+lDECFIjTjp9O8fRt0Ev9Dgwu6yVZbntnISCICEiqLk9nYyH7RjfKHBmdttsuX2UJQ+iIAAAcYqbnUns9FdC19o0Ah2s1yt8sK64AUEAVAh0kl7NBlPexoXNGi4PMuy7cFWLniAACkEGetufzidDlq4oEHDfr4sN7vCOucDQBAnVDpqdwf3s1EPFzRoFOuXbHMoqsqHACEJASkq0mn7djx76EU4o8FJyFevy4+DrZyXIAAUahStorjTHSweBinOaHDiPrPn9a6wlQshoEYQNaV10u70Hx5HbZzR4KTavj2/59ZWPoSAmkJDaZWk7f50Nu3ijAa1UG2fXzZ7a50P8ISQFBAgdZK2epPFtKvRoEEtlO9Pb5t9WTovAUKAxAkVddq6mfyY9iI0aFDz5er59XNfWReCoEZQKADJKEl748Wsr9GgQS0cV09vH8eydCHghKAAEBI6bvXu5o/DCA0a1Pxx9fSaF6UNQQQgAApOhDpNOnfzxTBGgwY1Z7Ofy7woKxcEAAFCAAgIlSTdwXwxTNGgQc0V2VOWF1XlgggIAhBChICO0s7g8ccoQYMGNX9Y/bfKj1XlvKBGCAEBCKgo7fQfF+MUDRoAQYrlU7Y/lqUPASdEQyhUSdwezH6MUzRoUHPF8ik7WFs5L2gQF2SctvuPi1ELDRoAIeyXz6u9rUrnBQICIGpCUEVppz9bjFM0aFBz+ern+96WVRUCLiggQOgobfdni3GKBg0AH4q359XhWJXOCwQgzghQx0lrOFuMUjRoAARfLJ9WxdFWLgQIcUIBCFLFabv/OB+laNAACL5YPq/2trIuBAjRoICg0jrtDGaLuwQNGtTcYfVzvbdlVYUg+I01HaWdweP8LkaDBkCQ4/J5eTiWpfdB8IUAqVSctkaz+TBGgwY1b7On5eFoSxcCBBcESKXitDOcPw5iNGhQ83b1nH3asqxCgOCMQpBKR2n3bv7Yj9CgQS3Y9XO2K2zpfBB8IUAqHbW6o/nDQKNBgxP7/vKSH6x1XgQQ4oyE0knaHc9ntxoNGtSC27y87g6FdSGIEGcEoKiTtDeeP9xEaNDgpNpmL5tDYSsvIiBOKAShoji9nSymXY0GDU5cvnzd7A62Cl7whQCUiqJ2fzKfdhQaNDjxh/XL+0dRVN4LvlAI6jju3E7nkw7OaHASjptsucnL0oUQiDMKqHXU6txO53dtnNGgcdyt3tb53jvvBYAQNUIplbS6w+miH+OMBg1/yNbLz31ZOR8ACAFRoqCjuNOb3E9vNc5o0PB2u3nfbA+l8yICQAhAKR2n3cF4OrrBBQ3OXLHdvObvR++CFwiFIFSko27v7n7ST3FBg4vqsNlkq/3ROQkMoihQWkWtm8FkOugqXNDgS5V/vK23O+sCnCgBlGKUtgf396N2rHBBg1+q/D1b5Z+lCwECEIqt7s3ofnSTanyhwW/VLvtYbQpXig8gVKpbveFocpsqhS80+KbMP9abrT2WgR5KJ61uf3Q3aCmFX2jwnd9/fOy2x6PzorRq9zp3vZtOjG9o8Adf7vO8OBYiUK1W+6bbTlJ8R4O/VKW1hQ9ORzpO2gkT/IEG/6jEuwBSMVLU+BMN/o+gAK/xNxpchwbXocF1aHAdGlyHBtehwXX+B1qf/ZfhapOCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAABICAAAAACVGCn3AAAFlElEQVRIDZXB6UIayxoF0L2ruukBEBDCoKL30evVojKJzFOP9V0tMDHJ+cNaNLgODa5Dg+vQ4Do0uA4NrkOD69DgOjT4V1mWAih6Gv+gwd/SPM1Ka8WreJHv4y80+FOeHLeHLC0s/DCo1+LQwx9o8IfsuF6tt/vCovSDWnTbrlcr+I4G32W7xftyfUjyzIPScdRsD1pxgG9o8E2xXc5m60OalkIgUJVaqzto1X38RoPfiv1iOt5ukzJHoZTQV3HUHvRbdR+/0OAXe1i9zBa7pCitCEB4yg9u2oNBK/bwhQa/nFaT18U2SwtrxSohtPJ00Bz0Bq0IX2jwpdi8vb6tTklhUQogpIanK7Xm3f2PmocLGnw5zMeT931aFFYEAkIUtaqEzU5/0AlxQYOLbD0bTzZZmou1QgEhVEr7QdToP/TrCmc0uNi9j6brXZKV1oIiFFoNzUrFv+nc37cCnNHgrNhOxvPVKckLfBDBJ2rl66DWehjexjijwVmyGE8Wu1OWWxEAloAoKqV8P27ePXarCg4Nzvbz0WR9POaFtYBAKBRS0dd+WL8b3t0oODRwyv30dbY5ZWkuAgEEHwiltO8FN52nQcuHQwOn3I5eFrtjmpXWQgAIAVBR6yCIuk/3tz4cGjjFcvy62J/SvLAQAAJAFEHl+2GtNXxsV+DQwEmXo/Fin56KUiwAgUNSeZUw7Nw/dQI4NHBOi9F4eTxlmbUCCoQCECR9L6y27ofdCA4NnNPiebw5JGluUVLgCKGgtB9Etw/DfgiHBs5x/jxbnU5ZWVoIAQEgIEHPC6uN+6duDIcGzmH2Mlsfs6ywIgAEgAAElPajoN3/XzeGQwNnP3uZbZIkz+UDBYBQICR1Jag2B8N+DIcGzv7t53SXJHlhYeEIACGVqgRR8/5hEMGhgbOf/pxvkyQvrcUnocCh9sO4cffUjeHQwNlOnt93SVoU1kJACAABQHp+GN72H3sRHBo4m/HrfJ+mWWEhgjMBQKX9oNrsD3sxHBo46/Hz/JQmeWFFIAQEnwit/CBuDob9CA4NPtn1+GWxT9O8tBZCoeCMSvthfDN47MVwaPDJrl5flsckLUprBWcCgFTaj8JW77EXw6HBp3I5el0d0jQrrQjOBACpdCWoNnoP/SocGnyyy9Hz4pQmZWFF8EEo+EBqVQniRv9hEMOhwadyNXpd7NO0KK0ILgQElfajuD4Y9qpwaOCsnl+XhyQrihKCCwFA5flR2OoNezEcGjjLyc/VPk2z0orgQgBQ60pQbXSHvSocGji78fPb8ZTmhRUBKPggAKG9SqXaGNz3Izg0cHbT0WyXJFlhRXAhAJTyvKhWu3/oRnBo4Jxmr2+bU5LlpQhACCgiJLXnR+Ht4KEbwKGBc1qMpttDkqZiRQgIIUIhPc8Pq+3esB3CoYGTr0eT9TY/ZbkViBIIICDpe4Ff7wz7tz4cGjjlYTp92xzypLBWKABECFFaBUFw0xne3Wg4NDhL3kdv611+ynNYOCKEUn7gR7WHx16MMxqcpfv5ZL4+ZHlhrQBCEYLKq+ig1n7q3wY4o8HFcfE2m++PaWlLCwoFFHqeDqJm/7FbUzijwUWRzOajxS4tylJECCGFnhfG1e7grhngggZf8u3bbLLZZ3kpFrBQCtRB0Lgd/mhHChc0+CXbTEZvu2NeWFuCoqipg0ane9eMQ3yhwW/JYjJ932RZXuqCVPSUV+t0h626h19o8M1hOZ1PjlmeQXSpAs+LG3e927qP32jw3W4/nm73SVJSAlb8qNH+0bnR+IYG35XpdrrebrOi9OiH0U29c1P38B0N/mDT/XZ7PBaF1kE1rNbjKv5Eg7/kaZKeWMDTkR+H+BsN/mZVVlirxfM8/IsG/8EqWIX/RIPr0OA6NLgODa5Dg+vQ4Do0uM7/AdeEAnalwNAMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "img_list = glob.glob('./submission_pred/*.png')[:5]\n",
    "\n",
    "for img_path in img_list:\n",
    "    display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdcb9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kji/workspace/jupyter_kji\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!zip -r -q submission_pred.zip ./submission_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "727ddb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x  2 kji kji   1044480 12월 28 14:19 submission_pred\r\n",
      "-rw-rw-r--  1 kji kji  43993228 12월 28 14:27 submission_pred.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l | grep submission_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8f92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
